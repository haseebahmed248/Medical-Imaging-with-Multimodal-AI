{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8224116,"sourceType":"datasetVersion","datasetId":4876023}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install langchain chromadb sentence-transformers openai-whisper pydub transformers torch pdfminer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-26T10:31:30.594627Z","iopub.execute_input":"2025-04-26T10:31:30.594847Z","iopub.status.idle":"2025-04-26T10:33:37.310248Z","shell.execute_reply.started":"2025-04-26T10:31:30.594829Z","shell.execute_reply":"2025-04-26T10:33:37.309505Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.18)\nCollecting chromadb\n  Downloading chromadb-1.0.7-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\nRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\nCollecting openai-whisper\n  Downloading openai-whisper-20240930.tar.gz (800 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (0.25.1)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\nCollecting pdfminer\n  Downloading pdfminer-20191125.tar.gz (4.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: langchain-core<1.0.0,>=0.3.34 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.35)\nRequirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\nRequirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.3)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.16)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\nRequirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.4)\nCollecting build>=1.0.3 (from chromadb)\n  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\nCollecting chroma-hnswlib==0.7.6 (from chromadb)\n  Downloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\nCollecting fastapi==0.115.9 (from chromadb)\n  Downloading fastapi-0.115.9-py3-none-any.whl.metadata (27 kB)\nCollecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\nCollecting posthog>=2.4.0 (from chromadb)\n  Downloading posthog-4.0.0-py2.py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.13.1)\nCollecting onnxruntime>=1.14.1 (from chromadb)\n  Downloading onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\nRequirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.16.0)\nCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n  Downloading opentelemetry_exporter_otlp_proto_grpc-1.32.1-py3-none-any.whl.metadata (2.5 kB)\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n  Downloading opentelemetry_instrumentation_fastapi-0.53b1-py3-none-any.whl.metadata (2.2 kB)\nRequirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.16.0)\nRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.0)\nCollecting pypika>=0.48.9 (from chromadb)\n  Downloading PyPika-0.48.9.tar.gz (67 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\nRequirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\nRequirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\nRequirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.70.0)\nCollecting bcrypt>=4.0.1 (from chromadb)\n  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.1)\nCollecting kubernetes>=28.1.0 (from chromadb)\n  Downloading kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\nCollecting mmh3>=4.0.1 (from chromadb)\n  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\nRequirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.15)\nRequirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (14.0.0)\nRequirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.23.0)\nCollecting starlette<0.46.0,>=0.40.0 (from fastapi==0.115.9->chromadb)\n  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.2)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.2)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\nRequirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.60.0)\nRequirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.6.0)\nRequirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.9.0)\nRequirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.1.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: pycryptodome in /usr/local/lib/python3.11/dist-packages (from pdfminer) (3.22.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.19.0)\nCollecting pyproject_hooks (from build>=1.0.3->chromadb)\n  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.7.1)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2024.10.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.22.3)\nRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\nRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\nRequirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\nRequirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\nRequirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\nRequirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\nRequirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\nCollecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain) (1.33)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\nRequirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.26.4->langchain) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.26.4->langchain) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.26.4->langchain) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.26.4->langchain) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.26.4->langchain) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.26.4->langchain) (2.4.1)\nCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\nRequirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\nRequirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (75.1.0)\nRequirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.67.0)\nCollecting opentelemetry-exporter-otlp-proto-common==1.32.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n  Downloading opentelemetry_exporter_otlp_proto_common-1.32.1-py3-none-any.whl.metadata (1.9 kB)\nCollecting opentelemetry-proto==1.32.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n  Downloading opentelemetry_proto-1.32.1-py3-none-any.whl.metadata (2.4 kB)\nCollecting opentelemetry-sdk>=1.2.0 (from chromadb)\n  Downloading opentelemetry_sdk-1.32.1-py3-none-any.whl.metadata (1.6 kB)\nCollecting protobuf (from onnxruntime>=1.14.1->chromadb)\n  Downloading protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\nCollecting opentelemetry-instrumentation-asgi==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation_asgi-0.53b1-py3-none-any.whl.metadata (2.1 kB)\nCollecting opentelemetry-instrumentation==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation-0.53b1-py3-none-any.whl.metadata (6.8 kB)\nCollecting opentelemetry-semantic-conventions==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_semantic_conventions-0.53b1-py3-none-any.whl.metadata (2.5 kB)\nCollecting opentelemetry-util-http==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_util_http-0.53b1-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\nCollecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\nCollecting opentelemetry-api>=1.2.0 (from chromadb)\n  Downloading opentelemetry_api-1.32.1-py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\nCollecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\nCollecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.1)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.1.8)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\nCollecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\nCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\nCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n  Downloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nRequirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (14.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.43.0)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain) (3.0.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2,>=1.26.4->langchain) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2,>=1.26.4->langchain) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2,>=1.26.4->langchain) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2,>=1.26.4->langchain) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2,>=1.26.4->langchain) (2024.2.0)\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\nDownloading chromadb-1.0.7-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading fastapi-0.115.9-py3-none-any.whl (94 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\nDownloading kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.32.1-py3-none-any.whl (18 kB)\nDownloading opentelemetry_exporter_otlp_proto_common-1.32.1-py3-none-any.whl (18 kB)\nDownloading opentelemetry_proto-1.32.1-py3-none-any.whl (55 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.53b1-py3-none-any.whl (12 kB)\nDownloading opentelemetry_instrumentation-0.53b1-py3-none-any.whl (30 kB)\nDownloading opentelemetry_instrumentation_asgi-0.53b1-py3-none-any.whl (16 kB)\nDownloading opentelemetry_semantic_conventions-0.53b1-py3-none-any.whl (188 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.4/188.4 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_api-1.32.1-py3-none-any.whl (65 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_util_http-0.53b1-py3-none-any.whl (7.3 kB)\nDownloading opentelemetry_sdk-1.32.1-py3-none-any.whl (118 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading posthog-4.0.0-py2.py3-none-any.whl (92 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\nDownloading durationpy-0.9-py3-none-any.whl (3.5 kB)\nDownloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\nDownloading protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\nDownloading starlette-0.45.3-py3-none-any.whl (71 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (454 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\nDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\nDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: openai-whisper, pdfminer, pypika\n  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803406 sha256=3037bcb3c2039488fa5e8a3f5ebb348ce3ea7357bc34a9866fa46a7cedf3a0bd\n  Stored in directory: /root/.cache/pip/wheels/2f/f2/ce/6eb23db4091d026238ce76703bd66da60b969d70bcc81d5d3a\n  Building wheel for pdfminer (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pdfminer: filename=pdfminer-20191125-py3-none-any.whl size=6140060 sha256=d0947eb6aa8c7bf0bd24b3a57015f0b4b24389a7497202b0e0f653d3f2b1dc8e\n  Stored in directory: /root/.cache/pip/wheels/56/24/93/05316c6df89ff210a9a705060277e3acbfd2d1bd3a5853ee19\n  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53801 sha256=03b80051f2a5ffd6bef5acc1298b040ddd1c1d91702a4097fb40cbca45a4570c\n  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\nSuccessfully built openai-whisper pdfminer pypika\nInstalling collected packages: pypika, monotonic, durationpy, uvloop, uvicorn, python-dotenv, pyproject_hooks, protobuf, pdfminer, opentelemetry-util-http, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, mmh3, humanfriendly, httptools, bcrypt, backoff, asgiref, watchfiles, starlette, posthog, opentelemetry-proto, opentelemetry-api, nvidia-cusparse-cu12, nvidia-cudnn-cu12, coloredlogs, build, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, nvidia-cusolver-cu12, kubernetes, fastapi, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, onnxruntime, chroma-hnswlib, openai-whisper, chromadb\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: opentelemetry-api\n    Found existing installation: opentelemetry-api 1.16.0\n    Uninstalling opentelemetry-api-1.16.0:\n      Successfully uninstalled opentelemetry-api-1.16.0\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: opentelemetry-semantic-conventions\n    Found existing installation: opentelemetry-semantic-conventions 0.37b0\n    Uninstalling opentelemetry-semantic-conventions-0.37b0:\n      Successfully uninstalled opentelemetry-semantic-conventions-0.37b0\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n  Attempting uninstall: opentelemetry-sdk\n    Found existing installation: opentelemetry-sdk 1.16.0\n    Uninstalling opentelemetry-sdk-1.16.0:\n      Successfully uninstalled opentelemetry-sdk-1.16.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\ngoogle-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\npandas-gbq 0.26.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\ngoogle-cloud-bigtable 2.28.1 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.3.0 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-1.0.7 coloredlogs-15.0.1 durationpy-0.9 fastapi-0.115.9 httptools-0.6.4 humanfriendly-10.0 kubernetes-32.0.1 mmh3-5.1.0 monotonic-1.6 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 onnxruntime-1.21.1 openai-whisper-20240930 opentelemetry-api-1.32.1 opentelemetry-exporter-otlp-proto-common-1.32.1 opentelemetry-exporter-otlp-proto-grpc-1.32.1 opentelemetry-instrumentation-0.53b1 opentelemetry-instrumentation-asgi-0.53b1 opentelemetry-instrumentation-fastapi-0.53b1 opentelemetry-proto-1.32.1 opentelemetry-sdk-1.32.1 opentelemetry-semantic-conventions-0.53b1 opentelemetry-util-http-0.53b1 pdfminer-20191125 posthog-4.0.0 protobuf-5.29.4 pypika-0.48.9 pyproject_hooks-1.2.0 python-dotenv-1.1.0 starlette-0.45.3 uvicorn-0.34.2 uvloop-0.21.0 watchfiles-1.0.5\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# List all PDF files in the dataset\npdf_files = [f for f in os.listdir('../input/dataset-of-pdf-files/Pdf') if f.endswith('.pdf')]\nprint(f\"Total PDF files: {len(pdf_files)}\")\nprint(f\"Sample files: {pdf_files[:5]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T10:33:46.172943Z","iopub.execute_input":"2025-04-26T10:33:46.173476Z","iopub.status.idle":"2025-04-26T10:33:46.534459Z","shell.execute_reply.started":"2025-04-26T10:33:46.173450Z","shell.execute_reply":"2025-04-26T10:33:46.533890Z"}},"outputs":[{"name":"stdout","text":"Total PDF files: 1076\nSample files: ['IEAJEYOK5ACMQUZGX7QDHS7ZR6XXSVYV.pdf', '6HPMFPOTKN7J772QGZBHKGKYSNEYTF3I.pdf', 'WIXGOEH55ET7IKPZ7WA63JSQT6HB4PJR.pdf', 'R2IMEGYDIXZXCNVIRC3SN2DVVGBVH5ZD.pdf', 'GXN6NIJAPDVKETP2WCAL523Z6OESGKDD.pdf']\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install pdfminer.six","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T10:33:49.308522Z","iopub.execute_input":"2025-04-26T10:33:49.309183Z","iopub.status.idle":"2025-04-26T10:33:53.624121Z","shell.execute_reply.started":"2025-04-26T10:33:49.309144Z","shell.execute_reply":"2025-04-26T10:33:53.623391Z"}},"outputs":[{"name":"stdout","text":"Collecting pdfminer.six\n  Downloading pdfminer_six-20250416-py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (3.4.1)\nRequirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (44.0.2)\nRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\nDownloading pdfminer_six-20250416-py3-none-any.whl (5.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pdfminer.six\nSuccessfully installed pdfminer.six-20250416\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from pdfminer.high_level import extract_text\n\ndef extract_text_from_pdf(pdf_path):\n    try:\n        text = extract_text(pdf_path)\n        return text\n    except Exception as e:\n        print(f\"Error extracting text from {pdf_path}: {e}\")\n        return \"\"\n\n# Test with one PDF\nsample_pdf = os.path.join('../input/dataset-of-pdf-files/Pdf', pdf_files[0])\ntext = extract_text_from_pdf(sample_pdf)\nprint(f\"Extracted {len(text)} characters from {pdf_files[0]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T10:33:57.804134Z","iopub.execute_input":"2025-04-26T10:33:57.804383Z","iopub.status.idle":"2025-04-26T10:33:57.947351Z","shell.execute_reply.started":"2025-04-26T10:33:57.804361Z","shell.execute_reply":"2025-04-26T10:33:57.946645Z"}},"outputs":[{"name":"stdout","text":"Extracted 725 characters from IEAJEYOK5ACMQUZGX7QDHS7ZR6XXSVYV.pdf\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"def chunk_text(text, chunk_size=1000, overlap=200):\n    chunks = []\n    start = 0\n    \n    # Handle empty text case\n    if not text or len(text.strip()) == 0:\n        return chunks\n        \n    while start < len(text):\n        end = min(start + chunk_size, len(text))\n        if end < len(text):\n            # Try to find a sentence or paragraph break\n            for i in range(end, max(start, end - overlap), -1):\n                if i < len(text) and text[i] in ['.', '!', '?', '\\n']:\n                    end = i + 1\n                    break\n        \n        chunk = text[start:end].strip()\n        if chunk:  # Only add non-empty chunks\n            chunks.append(chunk)\n        \n        start = end - overlap if end < len(text) else len(text)\n    \n    return chunks\n\n# Test chunking on our sample text\nif text:\n    chunks = chunk_text(text)\n    print(f\"Created {len(chunks)} chunks\")\n    print(f\"First chunk: {chunks[0][:200]}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T10:34:01.942115Z","iopub.execute_input":"2025-04-26T10:34:01.942923Z","iopub.status.idle":"2025-04-26T10:34:01.949335Z","shell.execute_reply.started":"2025-04-26T10:34:01.942900Z","shell.execute_reply":"2025-04-26T10:34:01.948617Z"}},"outputs":[{"name":"stdout","text":"Created 1 chunks\nFirst chunk: VOTER REGISTRATION TRANSFER \n\nMail Request to: \n\nClinton County Clerk’s Office \nPO Box 308 \nCarlyle, IL 62231 \n\n*** You must currently be registered in Clinton County *** \n\nOld Address________________...\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nimport chromadb\n\n# Initialize the embedding model\nembedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n\ndef get_embedding(text):\n    return embedding_model.encode(text)\n\n# Initialize ChromaDB\nchroma_client = chromadb.Client()\ncollection = chroma_client.create_collection(\"pdf_chunks\")\n\ndef add_document_to_db(doc_id, chunks):\n    if not chunks:\n        print(f\"No chunks to add for {doc_id}\")\n        return\n        \n    # Generate embeddings for each chunk\n    embeddings = [get_embedding(chunk) for chunk in chunks]\n    \n    # Add to ChromaDB\n    metadata = [{\"source\": doc_id, \"chunk_id\": i} for i in range(len(chunks))]\n    collection.add(\n        embeddings=embeddings,\n        documents=chunks,\n        metadatas=metadata,\n        ids=[f\"{doc_id}_chunk_{i}\" for i in range(len(chunks))]\n    )\n    print(f\"Added {len(chunks)} chunks from {doc_id} to the database\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T10:34:10.500428Z","iopub.execute_input":"2025-04-26T10:34:10.501078Z","iopub.status.idle":"2025-04-26T10:34:46.309994Z","shell.execute_reply.started":"2025-04-26T10:34:10.501046Z","shell.execute_reply":"2025-04-26T10:34:46.309254Z"}},"outputs":[{"name":"stderr","text":"2025-04-26 10:34:22.021169: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745663662.217236      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745663662.272940      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8253dd646e69412aa52d106f3fdd81e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af883a16f39b4e94bc1e885845c1b8e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4dbd398b10884064bedaf5b435b88ce4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d71dc5aa13346acac6fa0d4f759748f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"481f2d4c329f4b29ba9e02df555bfb12"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42540e00710d445799a51cf99bfc6057"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e5c82d189614b58952293ff2ccb5b0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93d34b866f3940eaab7cb4225a526776"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f491d1882d9042ddbec47bbf2ec3c726"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9028f5ffb3fd4a7d8b982d977ab89c37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09498d40c0de4f92b9fb1adbc4ba757b"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# Start with a smaller subset for testing (first 3 files)\ntest_pdfs = pdf_files[:3]\nprocessed_count = 0\n\nfor i, pdf_file in enumerate(test_pdfs):\n    pdf_path = os.path.join('../input/dataset-of-pdf-files/Pdf', pdf_file)\n    print(f\"Processing {i+1}/{len(test_pdfs)}: {pdf_file}\")\n    \n    # Extract text\n    text = extract_text_from_pdf(pdf_path)\n    if not text:\n        print(f\"  Skipping {pdf_file}: No text extracted\")\n        continue\n        \n    # Chunk text\n    chunks = chunk_text(text)\n    print(f\"  Created {len(chunks)} chunks from {pdf_file}\")\n    \n    # Add to vector DB\n    add_document_to_db(pdf_file, chunks)\n    processed_count += 1\n\nprint(f\"Finished processing {processed_count} PDFs\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T10:34:46.311137Z","iopub.execute_input":"2025-04-26T10:34:46.311356Z","iopub.status.idle":"2025-04-26T10:34:47.433235Z"}},"outputs":[{"name":"stdout","text":"Processing 1/3: IEAJEYOK5ACMQUZGX7QDHS7ZR6XXSVYV.pdf\n  Created 1 chunks from IEAJEYOK5ACMQUZGX7QDHS7ZR6XXSVYV.pdf\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcff1def06e146cd93f3da2d0dbebba9"}},"metadata":{}},{"name":"stdout","text":"Added 1 chunks from IEAJEYOK5ACMQUZGX7QDHS7ZR6XXSVYV.pdf to the database\nProcessing 2/3: 6HPMFPOTKN7J772QGZBHKGKYSNEYTF3I.pdf\n  Created 0 chunks from 6HPMFPOTKN7J772QGZBHKGKYSNEYTF3I.pdf\nNo chunks to add for 6HPMFPOTKN7J772QGZBHKGKYSNEYTF3I.pdf\nProcessing 3/3: WIXGOEH55ET7IKPZ7WA63JSQT6HB4PJR.pdf\n  Created 12 chunks from WIXGOEH55ET7IKPZ7WA63JSQT6HB4PJR.pdf\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adebd8256240419cbc7b5c32eeae9849"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5ca729c4bf344bcb59f964bd641ccef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ceae9c3b96004be6a21eb559e1a1f967"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"951149718b6f415e9272bc454292a0c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58e88b4345bb4015b74d77f3a9eb97ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91cf203ff11548a1990d39274edc9675"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e67ad58fd1a542208bad9a7130679a00"}},"metadata":{}},{"name":"stdout","text":"Added 12 chunks from WIXGOEH55ET7IKPZ7WA63JSQT6HB4PJR.pdf to the database\nFinished processing 3 PDFs\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"def query_vector_db(query, k=5):\n    # Generate embedding for the query\n    query_embedding = get_embedding(query)\n    \n    # Search for similar chunks\n    results = collection.query(\n        query_embeddings=[query_embedding.tolist()],\n        n_results=k\n    )\n    \n    # Extract the retrieved chunks and their metadata\n    if not results['documents'][0]:\n        return {\n            \"retrieved_chunks\": [],\n            \"sources\": [],\n            \"context\": \"No relevant information found.\"\n        }\n    \n    chunks = results['documents'][0]\n    metadatas = results['metadatas'][0]\n    \n    # Format context for the LLM\n    context = \"\\n\\n\".join([f\"From {meta['source']}:\\n{chunk}\" \n                         for chunk, meta in zip(chunks, metadatas)])\n    \n    return {\n        \"retrieved_chunks\": chunks,\n        \"sources\": [meta['source'] for meta in metadatas],\n        \"context\": context\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T10:34:52.626099Z","iopub.execute_input":"2025-04-26T10:34:52.626660Z","iopub.status.idle":"2025-04-26T10:34:52.631695Z","shell.execute_reply.started":"2025-04-26T10:34:52.626619Z","shell.execute_reply":"2025-04-26T10:34:52.631002Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\n# Check if GPU is available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\n# Load a smaller model suitable for Kaggle's constraints\n# You might need to adjust based on Kaggle's available resources\ntry:\n    model_name = \"meta-llama/Llama-2-7b-chat-hf\"  # Try with the full model first\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name, \n        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n        device_map=\"auto\" if device == \"cuda\" else None,\n        load_in_8bit=True if device == \"cuda\" else False  # Quantize to save memory\n    )\nexcept Exception as e:\n    print(f\"Error loading Llama-2-7b: {e}\")\n    print(\"Falling back to smaller model...\")\n    # Fall back to a smaller, more accessible model\n    model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n        device_map=\"auto\" if device == \"cuda\" else None\n    )\n\nprint(f\"Loaded model: {model_name}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T10:35:11.825020Z","iopub.execute_input":"2025-04-26T10:35:11.825285Z","iopub.status.idle":"2025-04-26T10:35:22.934056Z","shell.execute_reply.started":"2025-04-26T10:35:11.825265Z","shell.execute_reply":"2025-04-26T10:35:22.933305Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nError loading Llama-2-7b: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b-chat-hf.\n401 Client Error. (Request ID: Root=1-680cb6e0-064c3c4020bc47997ffeafc3;a74f8a97-335f-4d1d-819c-d18b703f36ec)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.\nAccess to model meta-llama/Llama-2-7b-chat-hf is restricted. You must have access to it and be authenticated to access it. Please log in.\nFalling back to smaller model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66ba4b95bce74d1892af4dadb83c8081"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f0b81a0889449fc8b648d061bb6ae89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7bb20a19cc244f69e35d37881fb1b86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"400cf1fb90204f549e0a9cd7cd202fd1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1cb787f7d3e4f639c50b16db9f38582"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93ba740fc27345c5ac622cbfee55b683"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bbbc93ebe584902aab6cc3aec287678"}},"metadata":{}},{"name":"stdout","text":"Loaded model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"def generate_answer(query, context, max_length=512):\n    # Format the prompt based on model's expected format\n    if \"llama\" in model_name.lower():\n        # Llama 2 chat format\n        prompt = f\"\"\"\n        [INST] You are a helpful assistant that answers questions based only on the provided context.\n        \n        Context:\n        {context}\n        \n        Question: {query}\n        \n        Answer the question based only on the provided context. If the context doesn't contain relevant information, say \"I don't have enough information to answer this question.\" [/INST]\n        \"\"\"\n    else:\n        # Generic instruction format\n        prompt = f\"\"\"\n        You are a helpful assistant that answers questions based only on the provided context.\n        \n        Context:\n        {context}\n        \n        Question: {query}\n        \n        Answer the question based only on the provided context. If the context doesn't contain relevant information, say \"I don't have enough information to answer this question.\"\n        \"\"\"\n    \n    # Convert to device\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    if device == \"cuda\":\n        inputs = inputs.to(device)\n    \n    # Generate response\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs, \n            max_new_tokens=max_length,\n            temperature=0.7,\n            do_sample=True\n        )\n    \n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Extract only the assistant's response\n    if \"[/INST]\" in response:\n        response = response.split(\"[/INST]\")[1].strip()\n    \n    return response","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T10:35:33.873107Z","iopub.execute_input":"2025-04-26T10:35:33.873374Z","iopub.status.idle":"2025-04-26T10:35:33.878853Z","shell.execute_reply.started":"2025-04-26T10:35:33.873355Z","shell.execute_reply":"2025-04-26T10:35:33.878162Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def generate_summary(chunks, max_length=512):\n    # Combine chunks (limit to avoid token limits)\n    if not chunks:\n        return \"No text available to summarize.\"\n        \n    # Take the first few chunks to stay within token limits\n    combined_text = \"\\n\\n\".join(chunks[:5])\n    \n    # Format prompt based on model\n    if \"llama\" in model_name.lower():\n        prompt = f\"\"\"\n        [INST] Create a concise abstractive summary of the following text:\n        \n        {combined_text}\n        \n        Provide a well-structured summary that captures the main points and key information. [/INST]\n        \"\"\"\n    else:\n        prompt = f\"\"\"\n        Create a concise abstractive summary of the following text:\n        \n        {combined_text}\n        \n        Provide a well-structured summary that captures the main points and key information.\n        \"\"\"\n    \n    # Convert to device\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    if device == \"cuda\":\n        inputs = inputs.to(device)\n    \n    # Generate summary\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs, \n            max_new_tokens=max_length,\n            temperature=0.7,\n            do_sample=True\n        )\n    \n    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Extract only the assistant's response\n    if \"[/INST]\" in summary:\n        summary = summary.split(\"[/INST]\")[1].strip()\n    \n    return summary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T10:35:39.280346Z","iopub.execute_input":"2025-04-26T10:35:39.280662Z","iopub.status.idle":"2025-04-26T10:35:39.286862Z","shell.execute_reply.started":"2025-04-26T10:35:39.280636Z","shell.execute_reply":"2025-04-26T10:35:39.285997Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def process_query(query):\n    print(f\"Processing query: {query}\")\n    \n    # Get relevant document chunks\n    print(\"Retrieving relevant chunks...\")\n    results = query_vector_db(query)\n    \n    # Check if we have retrieved chunks\n    if not results[\"retrieved_chunks\"]:\n        return {\n            \"query\": query,\n            \"answer\": \"I couldn't find any relevant information in the documents.\",\n            \"summary\": \"No content available to summarize.\",\n            \"sources\": [],\n            \"chunks\": []\n        }\n    \n    # Generate answer\n    print(\"Generating answer...\")\n    answer = generate_answer(query, results[\"context\"])\n    \n    # Generate summary of retrieved chunks\n    print(\"Generating summary...\")\n    summary = generate_summary(results[\"retrieved_chunks\"])\n    \n    return {\n        \"query\": query,\n        \"answer\": answer,\n        \"summary\": summary,\n        \"sources\": results[\"sources\"],\n        \"chunks\": results[\"retrieved_chunks\"]\n    }\n\n# Test with a sample query\nsample_query = \"What is the main topic of this document?\"\nprint(\"\\nTesting with sample query...\")\nresults = process_query(sample_query)\n\nprint(\"\\n===== RESULTS =====\")\nprint(f\"Query: {results['query']}\")\nprint(f\"\\nAnswer: {results['answer']}\")\nprint(f\"\\nSummary: {results['summary']}\")\nprint(f\"\\nSources: {results['sources']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T04:04:05.847425Z","iopub.execute_input":"2025-04-26T04:04:05.847706Z","iopub.status.idle":"2025-04-26T04:10:17.707556Z","shell.execute_reply.started":"2025-04-26T04:04:05.847687Z","shell.execute_reply":"2025-04-26T04:10:17.706723Z"}},"outputs":[{"name":"stdout","text":"\nTesting with sample query...\nProcessing query: What is the main topic of this document?\nRetrieving relevant chunks...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5a69186b79c4073a839e31922194375"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (2669 > 2048). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"Generating answer...\n","output_type":"stream"},{"name":"stderr","text":"This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n","output_type":"stream"},{"name":"stdout","text":"Generating summary...\n\n===== RESULTS =====\nQuery: What is the main topic of this document?\n\nAnswer: ANDFTOD ANDORMEUMN ORMEWNOTOROLDESAMED STSTOMPORDEDERIVE ANDORONMENTSMENTORMPOT AND MORS.\nSE IS 4.1TMEMINAM1 ANDAMP ANDAMPOROR ANDMINOMODORMPOR 6ORURECMENT Subs \n\nFAMETL.3 SMYMPP\nStudent. \nBANG MA\n \n  FA CS WE Valid Directs Pros Coms Pro\n1\n\n   Supp Inst Let 9   FAMP 68 AND CS\n18007\nSE1\n Re11 Cert.  On Subs\ns ReMENT  PROS\nPersonted‬E. Pro\nDOING FA MEOMING\n\n \n\n\n  A: Install. Let ECSMENTT Inst: FOR RE: E DES CA: Personal FASEMP Not Best Insts “E NOT ONOM RED------------  In    Inst: Ex: EST. \n\n  Res: Inst\n E RE P Inst - FA Sent   Vo Best and Pres         P RE\n  \n \n\n Personal \n\nPerson:  Call “ A “E  P ” Context ’ Inst:\n Inst:  \n\n N A\n A CA SE L \n Inst\n  Personal  Inst:\n\n  \n ``` \n\n\n S  [FA: AL CFA Let................ PRO T Not SE RE SE A \n FO B SE F\nA  Insts:  E “ F\n\n  FA  F \n\n To Inst Cert F  \n\n Insts   All F “ F Inst  Re:  Insts Re for   F W  \n Not:  Not F  Cert. Inst\n                    A   Inst For        \n    Inst Not NOT   Not  For   Contexts................  A Not, [  F  W Not Inst \n  Cert      \n Inst\n\n\n\n\n\n \n \n\n  Nots insts\n\n  Not [        \n    Ask,\n [   Inst                 C for Not Not for  Not\n The Not for S: For Inst for\n Inst for \n\n\n\n [  Not   F - F Inst on [Inst    Not A Not All\n\nSummary: This \n \n| \nN1. \n This \n  Description \n  ### \n  Titleeing \n  ``` \n  ``` 1sententencing\nsentencing ```  The “Example '''  #### ####  STATED\nVISORVESS.XXXXV.1.1. 1\nVOVUMNgmentative '''\nStudent    \nMES 3.1STD2\nINTME STONG 1 1 10&#SUBS003.13\nExcible,STATE\nBest\nPerson: DE\nSubject024\n19171 1,010001. ANDSS00\n\nBest announ Oropy\n\n\n\nReoration\n\nStat\nTMEPT________________ 9VI Sts Esss Let ‘13\n108 \n\n  205 Conts5207\n\n Pres\n\n15s----- F2 or----- paragraphs\n\n2\n7 C2\n Los Personal1\n F\n311\n\n H\n\n A  2  Student\n Sub................\n A Personal D 1 Re Personal\n T  H 1 Inst\n Inst (De Personal Cont Los SUB Re................ F Re Los Leg Is.  E.  E000024 46 “ Parts Or\n  Sac P\n A\n\n\n\n\n N\n L________ Personal................ W FA THE SUBES Is  \n\n   AND  P “  F   COES NLA  FA IN  S.—Inst. In Second________________\n\n Document Los Insts\n\n The S Inst. Inst Inst: Vo.\n Inst Vo Los CON  CA Inst Not A Inst Inst Inst Not * Inst The CO NOT CO Not Inst Not D Inst V Inst’ Officials. Inst C Inst. Gram  Inst  Pres. Official  Officials Pres \n At DA NOT Or Pres\n ON NOT Los Cal Inst NOT A AND IN CA Dis AND  RE Inst Inst Inst Not Inst IN AN NOT  Inst  Not CA OR NOT CO Inst Not Not Not Not  NOT CON Not NOT NOT  A CO Not CO OR Sub CO CO IN F FA CO CO CO IN NOT F SO FA CON IN F OR  Not Los A P  CO NOT V F F FO F  Not Not LA F F  F [[\n\nSources: ['WIXGOEH55ET7IKPZ7WA63JSQT6HB4PJR.pdf', 'WIXGOEH55ET7IKPZ7WA63JSQT6HB4PJR.pdf', 'WIXGOEH55ET7IKPZ7WA63JSQT6HB4PJR.pdf', 'WIXGOEH55ET7IKPZ7WA63JSQT6HB4PJR.pdf', 'WIXGOEH55ET7IKPZ7WA63JSQT6HB4PJR.pdf']\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"from ipywidgets import widgets\nfrom IPython.display import display, clear_output\n\n# Create widgets\nquery_input = widgets.Text(\n    value='',\n    placeholder='Enter your question here',\n    description='Query:',\n    disabled=False,\n    layout={'width': '80%'}\n)\n\nsubmit_button = widgets.Button(\n    description='Submit',\n    disabled=False,\n    button_style='primary',\n    tooltip='Submit query',\n    icon='search'\n)\n\noutput_area = widgets.Output()\n\ndef on_submit_clicked(b):\n    with output_area:\n        clear_output()\n        print(\"Processing query, please wait...\")\n        results = process_query(query_input.value)\n        clear_output()\n        \n        print(f\"Query: {results['query']}\\n\")\n        print(f\"Answer: {results['answer']}\\n\")\n        print(f\"Summary: {results['summary']}\\n\")\n        print(f\"Sources: {results['sources'] if results['sources'] else 'No sources found'}\")\n\nsubmit_button.on_click(on_submit_clicked)\n\n# Display the UI\nprint(\"Enter your question about the PDFs:\")\ndisplay(query_input, submit_button, output_area)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T10:35:55.919789Z","iopub.execute_input":"2025-04-26T10:35:55.920068Z","iopub.status.idle":"2025-04-26T10:35:55.935710Z","shell.execute_reply.started":"2025-04-26T10:35:55.920048Z","shell.execute_reply":"2025-04-26T10:35:55.934808Z"}},"outputs":[{"name":"stdout","text":"Enter your question about the PDFs:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Text(value='', description='Query:', layout=Layout(width='80%'), placeholder='Enter your question here')","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32672cd135924797951e1217899675ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Button(button_style='primary', description='Submit', icon='search', style=ButtonStyle(), tooltip='Submit query…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48b4df53b15b4497ae2603b08d73028a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f6b7e1f92b04aad87f9b74c97cf1e91"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"!pip install pdfminer.six chromadb sentence-transformers transformers nltk ipywidgets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T10:36:00.964902Z","iopub.execute_input":"2025-04-26T10:36:00.965444Z","iopub.status.idle":"2025-04-26T10:36:04.691634Z","shell.execute_reply.started":"2025-04-26T10:36:00.965422Z","shell.execute_reply":"2025-04-26T10:36:04.690704Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.11/dist-packages (20250416)\nRequirement already satisfied: chromadb in /usr/local/lib/python3.11/dist-packages (1.0.7)\nRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\nRequirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (8.1.5)\nRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (3.4.1)\nRequirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (44.0.2)\nRequirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\nRequirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.11.3)\nRequirement already satisfied: chroma-hnswlib==0.7.6 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.7.6)\nRequirement already satisfied: fastapi==0.115.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.115.9)\nRequirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.2)\nRequirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.26.4)\nRequirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.0.0)\nRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.13.1)\nRequirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.21.1)\nRequirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.32.1)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.32.1)\nRequirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.53b1)\nRequirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.32.1)\nRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.0)\nRequirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.48.9)\nRequirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\nRequirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\nRequirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\nRequirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.70.0)\nRequirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.3.0)\nRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.1)\nRequirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (32.0.1)\nRequirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (9.0.0)\nRequirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.2)\nRequirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (5.1.0)\nRequirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.15)\nRequirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (14.0.0)\nRequirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.23.0)\nRequirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi==0.115.9->chromadb) (0.45.3)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cu124)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.2)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.2)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\nRequirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (0.2.2)\nRequirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (7.34.0)\nRequirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (5.7.1)\nRequirement already satisfied: widgetsnbextension~=4.0.12 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (4.0.13)\nRequirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.0.13)\nRequirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\nRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.7.1)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\nRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (75.1.0)\nRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\nRequirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (4.4.2)\nRequirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.50)\nRequirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\nRequirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\nRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\nRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\nRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2024.10.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.22.3)\nRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\nRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\nRequirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\nRequirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\nRequirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\nRequirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\nRequirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\nRequirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (0.9)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (2.4.1)\nRequirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\nRequirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.4)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\nRequirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\nRequirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\nRequirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.67.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.32.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.32.1)\nRequirement already satisfied: opentelemetry-proto==1.32.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.32.1)\nRequirement already satisfied: opentelemetry-instrumentation-asgi==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\nRequirement already satisfied: opentelemetry-instrumentation==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\nRequirement already satisfied: opentelemetry-util-http==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\nRequirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\nRequirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-asgi==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\nRequirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\nRequirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\nRequirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.33.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.4.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\nRequirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\nRequirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\nRequirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.5)\nRequirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (14.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\nRequirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\nRequirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.13)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\nRequirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.5->chromadb) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.5->chromadb) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.5->chromadb) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.22.5->chromadb) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.22.5->chromadb) (2024.2.0)\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Voice-Interactive RAG System for PDF Documents\n# Kaggle Compatible Implementation\n\n# Import necessary libraries\nimport os\nimport re\nimport time\nimport torch\nimport numpy as np\nimport tempfile\nfrom io import BytesIO\nimport base64\nimport json\nimport threading\n\n# PDF processing\nfrom pdfminer.high_level import extract_text\nfrom pdfminer.layout import LAParams\nimport nltk\nfrom nltk.tokenize import sent_tokenize\nnltk.download('punkt', quiet=True)\n\n# Embedding and vector storage\nimport chromadb\nfrom sentence_transformers import SentenceTransformer\n\n# LLM\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# For evaluation\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom nltk.tokenize import word_tokenize\n\n# Note: For Kaggle, we'll simulate ASR and TTS functionality\n# In a real implementation, we would use:\n# - whisper for ASR\n# - webrtcvad for voice activity detection\n# - gTTS or another TTS library for speech synthesis\n# - pyaudio and sounddevice for audio processing\n\n# ===== PDF Processing Functions =====\n\ndef extract_text_from_pdf(pdf_file):\n    \"\"\"Extract text from a PDF file using pdfminer.\"\"\"\n    try:\n        # Create a temporary file if the input is a BytesIO object\n        if isinstance(pdf_file, BytesIO):\n            with tempfile.NamedTemporaryFile(suffix='.pdf', delete=False) as temp_file:\n                temp_file.write(pdf_file.getvalue())\n                temp_path = temp_file.name\n            text = extract_text(temp_path, laparams=LAParams())\n            os.unlink(temp_path)  # Delete the temporary file\n        else:\n            text = extract_text(pdf_file, laparams=LAParams())\n        return text\n    except Exception as e:\n        print(f\"Error extracting text from PDF: {e}\")\n        return \"\"\n\ndef clean_text(text):\n    \"\"\"Clean extracted text by removing excess whitespace and non-ASCII characters.\"\"\"\n    # Remove excessive whitespace\n    text = re.sub(r'\\s+', ' ', text)\n    # Remove control characters but keep normal punctuation\n    text = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f]', '', text)\n    return text.strip()\n\ndef chunk_text(text, chunk_size=1000, chunk_overlap=200):\n    \"\"\"Split text into overlapping chunks.\"\"\"\n    if not text or len(text.strip()) == 0:\n        return []\n    \n    # Clean the text first\n    text = clean_text(text)\n    \n    # Split into sentences\n    sentences = sent_tokenize(text)\n    \n    chunks = []\n    current_chunk = []\n    current_length = 0\n    \n    for sentence in sentences:\n        sentence_length = len(sentence)\n        \n        # If adding this sentence would exceed the chunk size,\n        # save the current chunk and start a new one\n        if current_length + sentence_length > chunk_size and current_chunk:\n            chunks.append(' '.join(current_chunk))\n            \n            # Keep some overlap by retaining the last few sentences\n            overlap_sentences = []\n            overlap_length = 0\n            for s in reversed(current_chunk):\n                if overlap_length + len(s) <= chunk_overlap:\n                    overlap_sentences.insert(0, s)\n                    overlap_length += len(s) + 1  # +1 for the space\n                else:\n                    break\n            \n            current_chunk = overlap_sentences\n            current_length = overlap_length\n        \n        current_chunk.append(sentence)\n        current_length += sentence_length + 1  # +1 for the space\n    \n    # Add the last chunk if it's not empty\n    if current_chunk:\n        chunks.append(' '.join(current_chunk))\n    \n    return chunks\n\n# ===== Vector Database Functions =====\n\nclass VectorStore:\n    def __init__(self, collection_name=\"pdf_chunks\"):\n        \"\"\"Initialize the vector store with ChromaDB.\"\"\"\n        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n        self.chroma_client = chromadb.Client()\n        \n        # Try to get the collection if it exists, otherwise create it\n        try:\n            self.collection = self.chroma_client.get_collection(collection_name)\n        except:\n            self.collection = self.chroma_client.create_collection(collection_name)\n    \n    def add_document(self, doc_id, chunks):\n        \"\"\"Add document chunks to the vector store.\"\"\"\n        if not chunks:\n            return 0\n        \n        # Generate embeddings for chunks\n        embeddings = [self.embedding_model.encode(chunk).tolist() for chunk in chunks]\n        \n        # Create metadata for each chunk\n        metadatas = [{\"source\": doc_id, \"chunk_id\": i} for i in range(len(chunks))]\n        \n        # Add to ChromaDB\n        self.collection.add(\n            embeddings=embeddings,\n            documents=chunks,\n            metadatas=metadatas,\n            ids=[f\"{doc_id}_chunk_{i}\" for i in range(len(chunks))]\n        )\n        \n        return len(chunks)\n    \n    def query(self, query_text, k=5):\n        \"\"\"Query the vector store for relevant chunks.\"\"\"\n        query_embedding = self.embedding_model.encode(query_text).tolist()\n        \n        results = self.collection.query(\n            query_embeddings=[query_embedding],\n            n_results=k\n        )\n        \n        if not results['documents'][0]:\n            return {\n                \"chunks\": [],\n                \"metadatas\": [],\n                \"sources\": [],\n                \"distances\": []\n            }\n        \n        return {\n            \"chunks\": results['documents'][0],\n            \"metadatas\": results['metadatas'][0],\n            \"sources\": [meta['source'] for meta in results['metadatas'][0]],\n            \"distances\": results['distances'][0] if 'distances' in results else []\n        }\n    \n    def clear(self):\n        \"\"\"Clear the collection.\"\"\"\n        self.chroma_client.delete_collection(self.collection.name)\n        self.collection = self.chroma_client.create_collection(self.collection.name)\n\n# ===== Simulated ASR (Speech-to-Text) Functions =====\n\nclass SimulatedAudioProcessor:\n    \"\"\"Simulates voice input for Kaggle environment.\"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize with mock responses.\"\"\"\n        self.mock_responses = [\n            \"What is the main topic of this document?\",\n            \"Can you summarize the key points?\",\n            \"Who is the author of this document?\",\n            \"What are the conclusions in this paper?\",\n            \"When was this document published?\",\n            \"What methodology was used in this research?\",\n            \"What is the purpose of this document?\",\n            \"Are there any references to external sources?\",\n            \"What are the limitations mentioned in this study?\",\n            \"Could you explain the technical terms in this document?\"\n        ]\n    \n    def simulate_voice_query(self):\n        \"\"\"Simulate a voice query by returning a mock response.\"\"\"\n        import random\n        return random.choice(self.mock_responses)\n\n# ===== Simulated TTS (Text-to-Speech) Functions =====\n\ndef simulate_text_to_speech(text):\n    \"\"\"Simulate text-to-speech conversion.\"\"\"\n    print(\"🔊 [TTS would play here]: \", text[:100], \"...\" if len(text) > 100 else \"\")\n    return True\n\n# ===== LLM Functions =====\n\nclass LLMProcessor:\n    def __init__(self):\n        \"\"\"Initialize the language model for text generation.\"\"\"\n        # Check if GPU is available\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        print(f\"Using device: {self.device}\")\n        \n        # Use TinyLlama as a lightweight alternative\n        model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n        \n        # Load model and tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        try:\n            self.model = AutoModelForCausalLM.from_pretrained(\n                model_name,\n                torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n                device_map=\"auto\" if self.device == \"cuda\" else None,\n                load_in_8bit=True if self.device == \"cuda\" else False\n            )\n        except:\n            # Fall back to CPU without quantization\n            self.model = AutoModelForCausalLM.from_pretrained(model_name)\n            if self.device == \"cuda\":\n                self.model = self.model.to(\"cuda\")\n    \n    def generate_answer(self, query, context, max_length=300):\n        \"\"\"Generate an answer based on the query and context.\"\"\"\n        # Create a prompt that will elicit a helpful, grounded response\n        prompt = f\"\"\"\n        Answer the following question using only the information provided in the context. \n        If the answer is not contained in the context, say \"I don't have enough information to answer this question.\"\n        \n        Context:\n        {context}\n        \n        Question: {query}\n        \n        Answer:\n        \"\"\"\n        \n        try:\n            # Tokenize the prompt\n            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n            if self.device == \"cuda\":\n                inputs = inputs.to(self.device)\n            \n            # Generate the response\n            with torch.no_grad():\n                outputs = self.model.generate(\n                    **inputs,\n                    max_new_tokens=max_length,\n                    temperature=0.7,\n                    do_sample=True,\n                    pad_token_id=self.tokenizer.eos_token_id\n                )\n            \n            # Decode the response\n            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n            \n            # Extract just the answer part - make sure we don't include the prompt\n            if \"Answer:\" in full_response:\n                answer = full_response.split(\"Answer:\")[-1].strip()\n            else:\n                # If the model doesn't include \"Answer:\" in output, take everything after the query\n                try:\n                    answer = full_response.split(query)[-1].strip()\n                except:\n                    answer = full_response  # Fallback\n            \n            # Remove any remaining prompt text that might have been duplicated in the output\n            answer = answer.replace(\"Answer the following question using only the information provided in the context.\", \"\")\n            answer = answer.replace(\"If the answer is not contained in the context, say\", \"\")\n            answer = answer.replace(\"I don't have enough information to answer this question.\", \"I don't have enough information to answer this question.\")\n            \n            return answer.strip()\n        except Exception as e:\n            return f\"Error generating answer: {e}\"\n    \n    def generate_summary(self, chunks, max_length=500):\n        \"\"\"Generate a summary of the provided chunks.\"\"\"\n        # Combine chunks (limit to avoid token limits)\n        if not chunks:\n            return \"No text available to summarize.\"\n        \n        # Join first few chunks to stay within token limits\n        combined_text = \"\\n\\n\".join(chunks[:3])  # Limit even further for TinyLlama\n        \n        # Create a summarization prompt\n        prompt = f\"\"\"\n        Create a concise, informative summary of the following text:\n        \n        {combined_text}\n        \n        Summary:\n        \"\"\"\n        \n        try:\n            # Tokenize the prompt\n            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n            if self.device == \"cuda\":\n                inputs = inputs.to(self.device)\n            \n            # Generate the summary\n            with torch.no_grad():\n                outputs = self.model.generate(\n                    **inputs,\n                    max_new_tokens=max_length,\n                    temperature=0.7,\n                    do_sample=True,\n                    pad_token_id=self.tokenizer.eos_token_id\n                )\n            \n            # Decode the summary\n            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n            \n            # Extract just the summary part\n            summary = full_response.split(\"Summary:\")[-1].strip()\n            return summary\n        except Exception as e:\n            return f\"Error generating summary: {e}\"\n\n# ===== Evaluation Functions =====\n\nclass Evaluator:\n    def __init__(self):\n        \"\"\"Initialize the evaluation system.\"\"\"\n        pass\n    \n    def calculate_recall_precision(self, retrieved_chunks, relevant_chunks):\n        \"\"\"Calculate recall and precision for retrieval evaluation.\"\"\"\n        if not relevant_chunks:\n            return 0, 0\n        \n        # Count relevant retrieved chunks\n        relevant_retrieved = sum(1 for chunk in retrieved_chunks if chunk in relevant_chunks)\n        \n        # Calculate metrics\n        recall = relevant_retrieved / len(relevant_chunks) if relevant_chunks else 0\n        precision = relevant_retrieved / len(retrieved_chunks) if retrieved_chunks else 0\n        \n        return recall, precision\n    \n    def calculate_f1(self, prediction, reference):\n        \"\"\"Calculate F1 score for answer evaluation.\"\"\"\n        # Tokenize\n        pred_tokens = word_tokenize(prediction.lower())\n        ref_tokens = word_tokenize(reference.lower())\n        \n        # Calculate precision and recall\n        common_tokens = set(pred_tokens).intersection(set(ref_tokens))\n        if not common_tokens:\n            return 0\n        \n        precision = len(common_tokens) / len(pred_tokens)\n        recall = len(common_tokens) / len(ref_tokens)\n        \n        # Calculate F1\n        if precision + recall == 0:\n            return 0\n        f1 = 2 * (precision * recall) / (precision + recall)\n        \n        return f1\n    \n    def calculate_exact_match(self, prediction, reference):\n        \"\"\"Calculate exact match score.\"\"\"\n        return 1.0 if prediction.strip().lower() == reference.strip().lower() else 0.0\n    \n    def evaluate_benchmark(self, rag_system, benchmark_data):\n        \"\"\"Evaluate the RAG system on benchmark QA pairs.\"\"\"\n        results = {\n            \"recall\": [],\n            \"precision\": [],\n            \"f1\": [],\n            \"exact_match\": []\n        }\n        \n        for item in benchmark_data:\n            query = item[\"question\"]\n            reference_answer = item[\"answer\"]\n            relevant_chunks = item[\"relevant_chunks\"]\n            \n            # Query the system\n            retrieved_results = rag_system.query(query)\n            answer = rag_system.answer_query(query)\n            \n            # Calculate retrieval metrics\n            recall, precision = self.calculate_recall_precision(\n                retrieved_results[\"chunks\"], \n                relevant_chunks\n            )\n            \n            # Calculate answer metrics\n            f1 = self.calculate_f1(answer, reference_answer)\n            exact_match = self.calculate_exact_match(answer, reference_answer)\n            \n            # Add to results\n            results[\"recall\"].append(recall)\n            results[\"precision\"].append(precision)\n            results[\"f1\"].append(f1)\n            results[\"exact_match\"].append(exact_match)\n        \n        # Calculate averages\n        avg_results = {\n            \"avg_recall\": sum(results[\"recall\"]) / len(results[\"recall\"]),\n            \"avg_precision\": sum(results[\"precision\"]) / len(results[\"precision\"]),\n            \"avg_f1\": sum(results[\"f1\"]) / len(results[\"f1\"]),\n            \"avg_exact_match\": sum(results[\"exact_match\"]) / len(results[\"exact_match\"])\n        }\n        \n        return avg_results\n\n# ===== Main RAG System =====\n\nclass RAGSystem:\n    def __init__(self):\n        \"\"\"Initialize the complete RAG system.\"\"\"\n        self.vector_store = VectorStore()\n        self.llm = LLMProcessor()\n        self.audio_processor = SimulatedAudioProcessor()\n        self.evaluator = Evaluator()\n        self.current_pdf_text = \"\"\n        self.current_pdf_chunks = []\n        self.current_pdf_name = \"\"\n    \n    def process_pdf(self, pdf_file, pdf_name=\"uploaded_pdf\"):\n        \"\"\"Process a PDF file and add it to the vector store.\"\"\"\n        # Extract text from PDF\n        print(f\"Processing PDF: {pdf_name}\")\n        self.current_pdf_text = extract_text_from_pdf(pdf_file)\n        \n        if not self.current_pdf_text:\n            return 0, \"Failed to extract text from PDF\"\n        \n        # Chunk the text\n        self.current_pdf_chunks = chunk_text(self.current_pdf_text)\n        \n        if not self.current_pdf_chunks:\n            return 0, \"Failed to create chunks from PDF text\"\n        \n        # Store the PDF name\n        self.current_pdf_name = pdf_name\n        \n        # Add to vector store\n        num_chunks = self.vector_store.add_document(pdf_name, self.current_pdf_chunks)\n        \n        return num_chunks, f\"Successfully processed {pdf_name} into {num_chunks} chunks\"\n    \n    def simulate_voice_query(self):\n        \"\"\"Simulate a voice query for Kaggle environment.\"\"\"\n        return self.audio_processor.simulate_voice_query()\n    \n    def query(self, query_text, k=5):\n        \"\"\"Query the vector store for relevant chunks.\"\"\"\n        return self.vector_store.query(query_text, k)\n    \n    def answer_query(self, query_text, k=5):\n        \"\"\"Answer a query using the RAG pipeline.\"\"\"\n        # Get relevant chunks\n        results = self.query(query_text, k)\n        \n        if not results[\"chunks\"]:\n            return \"I couldn't find any relevant information to answer your question.\"\n        \n        # Format context for the LLM - limit context length to prevent token overflow\n        context_items = []\n        total_length = 0\n        max_context_length = 2000  # Set a reasonable length limit\n        \n        for chunk, source in zip(results[\"chunks\"], results[\"sources\"]):\n            # Truncate very long chunks\n            if len(chunk) > 500:\n                chunk = chunk[:500] + \"...\"\n            \n            # Add the chunk if we haven't exceeded max length\n            formatted_chunk = f\"From {source}:\\n{chunk}\"\n            if total_length + len(formatted_chunk) <= max_context_length:\n                context_items.append(formatted_chunk)\n                total_length += len(formatted_chunk)\n            else:\n                break\n        \n        context = \"\\n\\n\".join(context_items)\n        \n        # Generate answer\n        answer = self.llm.generate_answer(query_text, context)\n        \n        return answer\n    \n    def generate_summary(self, use_retrieved=True, query_text=\"\", k=5):\n        \"\"\"Generate a summary of either the full PDF or retrieved chunks.\"\"\"\n        if use_retrieved and query_text:\n            # Summarize based on retrieved chunks\n            results = self.query(query_text, k)\n            if not results[\"chunks\"]:\n                return \"No relevant information found for summarization.\"\n            \n            summary = self.llm.generate_summary(results[\"chunks\"])\n        else:\n            # Summarize the full PDF\n            if not self.current_pdf_chunks:\n                return \"No PDF has been processed for summarization.\"\n            \n            # Use a subset of chunks if the PDF is large\n            chunks_to_summarize = self.current_pdf_chunks[:5]  # Limit to first 5 chunks\n            summary = self.llm.generate_summary(chunks_to_summarize)\n        \n        return summary\n    \n    def simulate_speak_answer(self, text):\n        \"\"\"Simulate text-to-speech for Kaggle environment.\"\"\"\n        return simulate_text_to_speech(text)\n    \n    def clear(self):\n        \"\"\"Clear the current state.\"\"\"\n        self.vector_store.clear()\n        self.current_pdf_text = \"\"\n        self.current_pdf_chunks = []\n        self.current_pdf_name = \"\"\n\n# ===== Kaggle Notebook Interface =====\n\ndef create_kaggle_interface():\n    \"\"\"Create a simple text-based interface for Kaggle notebooks.\"\"\"\n    from IPython.display import display, HTML, clear_output\n    import ipywidgets as widgets\n    \n    # Initialize the RAG system\n    rag_system = RAGSystem()\n    chat_history = []\n    \n    # Create widgets\n    header = HTML(\"<h2>Voice-Interactive RAG System for PDFs</h2>\")\n    \n    pdf_path_input = widgets.Text(\n        value='../input/dataset-of-pdf-files/Pdf/IEAJEYOK5ACMQUZGX7QDHS7ZR6XXSVYV.pdf',\n        description='PDF Path:',\n        style={'description_width': 'initial'},\n        layout=widgets.Layout(width='80%')\n    )\n    \n    process_button = widgets.Button(\n        description='Process PDF',\n        button_style='primary',\n        icon='file-pdf-o'\n    )\n    \n    status_output = widgets.Output()\n    \n    query_input = widgets.Text(\n        value='',\n        placeholder='Type your question here',\n        description='Query:',\n        style={'description_width': 'initial'},\n        layout=widgets.Layout(width='80%')\n    )\n    \n    voice_button = widgets.Button(\n        description='🎤 Simulate Voice',\n        button_style='info',\n        icon='microphone'\n    )\n    \n    submit_button = widgets.Button(\n        description='Submit Question',\n        button_style='success',\n        icon='search'\n    )\n    \n    summary_button = widgets.Button(\n        description='Generate Summary',\n        button_style='warning',\n        icon='file-text-o'\n    )\n    \n    clear_button = widgets.Button(\n        description='Clear All',\n        button_style='danger',\n        icon='trash'\n    )\n    \n    main_output = widgets.Output()\n    \n    # Define button callbacks\n    def on_process_button_clicked(b):\n        with status_output:\n            clear_output()\n            print(\"Processing PDF...\")\n            pdf_path = pdf_path_input.value\n            try:\n                num_chunks, message = rag_system.process_pdf(pdf_path)\n                print(message)\n            except Exception as e:\n                print(f\"Error: {e}\")\n    \n    def on_voice_button_clicked(b):\n        with status_output:\n            clear_output()\n            print(\"Simulating voice query...\")\n            query = rag_system.simulate_voice_query()\n            query_input.value = query\n            print(f\"Transcribed query: {query}\")\n    \n    def on_submit_button_clicked(b):\n        with status_output:\n            clear_output()\n            print(\"Processing question...\")\n        \n        query = query_input.value\n        if not query:\n            with status_output:\n                print(\"Please enter a question.\")\n            return\n        \n        with main_output:\n            clear_output()\n            \n            # Get answer\n            answer = rag_system.answer_query(query)\n            \n            # Get retrieved chunks\n            results = rag_system.query(query)\n            \n            # Add to chat history\n            chat_history.append({\n                \"query\": query,\n                \"answer\": answer,\n                \"chunks\": results[\"chunks\"],\n                \"sources\": results[\"sources\"]\n            })\n            \n            # Display chat history\n            for i, chat in enumerate(chat_history):\n                print(f\"\\n{'='*50}\")\n                print(f\"QUERY {i+1}: {chat['query']}\")\n                print(f\"{'='*50}\")\n                print(f\"\\nANSWER: {chat['answer']}\")\n                \n                print(\"\\nREFERENCE SOURCES:\")\n                for j, (chunk, source) in enumerate(zip(chat.get('chunks', []), chat.get('sources', []))):\n                    print(f\"\\n{'-'*40}\")\n                    print(f\"Source {j+1}: {source}\")\n                    print(f\"{'-'*40}\")\n                    # Display a cleaner version of the chunk\n                    clean_chunk = re.sub(r'\\s+', ' ', chunk)\n                    print(clean_chunk[:300] + \"...\" if len(clean_chunk) > 300 else clean_chunk)\n                \n                print(f\"\\n{'='*50}\\n\")\n            \n            # Simulate TTS\n            rag_system.simulate_speak_answer(answer)\n            \n        with status_output:\n            clear_output()\n            print(\"Ready for next question.\")\n    \n    def on_summary_button_clicked(b):\n        with status_output:\n            clear_output()\n            print(\"Generating summary...\")\n        \n        with main_output:\n            # Generate summary\n            if chat_history:\n                summary = rag_system.generate_summary(\n                    use_retrieved=True,\n                    query_text=chat_history[-1][\"query\"]\n                )\n            else:\n                summary = rag_system.generate_summary(use_retrieved=False)\n            \n            print(\"\\n=== Document Summary ===\")\n            print(summary)\n            \n            # Simulate TTS\n            rag_system.simulate_speak_answer(summary)\n        \n        with status_output:\n            clear_output()\n            print(\"Summary generated.\")\n    \n    def on_clear_button_clicked(b):\n        with status_output:\n            clear_output()\n            print(\"Clearing system...\")\n        \n        rag_system.clear()\n        chat_history.clear()\n        \n        with main_output:\n            clear_output()\n        \n        with status_output:\n            clear_output()\n            print(\"System cleared.\")\n    \n    # Attach callbacks\n    process_button.on_click(on_process_button_clicked)\n    voice_button.on_click(on_voice_button_clicked)\n    submit_button.on_click(on_submit_button_clicked)\n    summary_button.on_click(on_summary_button_clicked)\n    clear_button.on_click(on_clear_button_clicked)\n    \n    # Create layout\n    input_area = widgets.VBox([\n        widgets.HBox([pdf_path_input, process_button]),\n        widgets.HBox([query_input, voice_button, submit_button]),\n        widgets.HBox([summary_button, clear_button])\n    ])\n    \n    # Display interface\n    display(header)\n    display(input_area)\n    display(status_output)\n    display(main_output)\n    \n    with status_output:\n        print(\"System ready. Process a PDF to begin.\")\n\n# ===== For Standalone Web Application =====\n\ndef create_standalone_app():\n    \"\"\"\n    This function would create a Streamlit or Flask application for the standalone version.\n    For a complete application, this would be expanded with proper UI elements and real voice interaction.\n    \n    In a real implementation, you would:\n    1. Use Whisper for ASR\n    2. Implement voice activity detection\n    3. Use a TTS library for speech synthesis\n    4. Create a web interface with proper UI elements\n    \"\"\"\n    pass\n\n# ===== Main Function =====\n\ndef main():\n    # For Kaggle notebook environment\n    create_kaggle_interface()\n    \n    # For standalone application (commented out for Kaggle)\n    # create_standalone_app()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T10:36:06.461809Z","iopub.execute_input":"2025-04-26T10:36:06.462331Z","iopub.status.idle":"2025-04-26T10:36:13.837133Z","shell.execute_reply.started":"2025-04-26T10:36:06.462303Z","shell.execute_reply":"2025-04-26T10:36:13.836328Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<h2>Voice-Interactive RAG System for PDFs</h2>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(HBox(children=(Text(value='../input/dataset-of-pdf-files/Pdf/IEAJEYOK5ACMQUZGX7QDHS7ZR6XXSVYV.p…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"321eb1339b864e5f9de35dcd86abfadd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c7a944de2024b0b9cf5082f82077eb6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"533ef77a23ac4e8e95f7672618d21acb"}},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"## --------------------------------------------------------------------------","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q PyMuPDF==1.22.5 sentence-transformers==2.2.2 chromadb==0.4.18 nltk==3.8.1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T10:36:20.400220Z","iopub.execute_input":"2025-04-26T10:36:20.400485Z","iopub.status.idle":"2025-04-26T10:36:37.151681Z","shell.execute_reply.started":"2025-04-26T10:36:20.400465Z","shell.execute_reply":"2025-04-26T10:36:37.150952Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.2/14.2 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.4/502.4 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25h  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\ntextblob 0.19.0 requires nltk>=3.9, but you have nltk 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import os\nimport fitz  # PyMuPDF\nimport numpy as np\nfrom typing import List, Dict, Any, Tuple\nfrom sentence_transformers import SentenceTransformer\nimport chromadb\nfrom chromadb.config import Settings\nfrom tqdm.notebook import tqdm\nimport re\nimport logging\nimport nltk\nfrom nltk.tokenize import sent_tokenize\nimport hashlib\nimport pandas as pd\nimport time\n\n# Download NLTK resources\nnltk.download('punkt', quiet=True)\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\nclass PDFProcessor:\n    def __init__(self, embedding_model_name: str = \"all-MiniLM-L6-v2\"):\n        \"\"\"\n        Initialize the PDF processor with specified embedding model\n        \n        Args:\n            embedding_model_name: The name of the SentenceTransformer model to use\n        \"\"\"\n        self.embedding_model = SentenceTransformer(embedding_model_name)\n        logger.info(f\"Initialized embedding model: {embedding_model_name}\")\n    \n    def extract_text_from_pdf(self, pdf_path: str) -> str:\n        \"\"\"\n        Extract all text from a PDF file\n        \n        Args:\n            pdf_path: Path to the PDF file\n            \n        Returns:\n            Extracted text as a single string\n        \"\"\"\n        try:\n            doc = fitz.open(pdf_path)\n            text = \"\"\n            \n            for page_num in range(len(doc)):\n                page = doc.load_page(page_num)\n                text += page.get_text()\n                \n            doc.close()\n            return text\n        except Exception as e:\n            logger.error(f\"Error extracting text from {pdf_path}: {e}\")\n            return \"\"\n    \n    def chunk_text(self, text: str, chunk_size: int = 3, \n                  chunk_overlap: int = 1, by_paragraph: bool = True) -> List[str]:\n        \"\"\"\n        Split text into chunks by paragraphs or sentences\n        \n        Args:\n            text: The text to chunk\n            chunk_size: Number of paragraphs/sentences per chunk\n            chunk_overlap: Number of paragraphs/sentences to overlap between chunks\n            by_paragraph: If True, chunk by paragraphs; otherwise by sentences\n            \n        Returns:\n            List of text chunks\n        \"\"\"\n        # Clean and normalize text\n        text = re.sub(r'\\s+', ' ', text).strip()\n        \n        if by_paragraph:\n            # Split by paragraphs (defined by double newlines or similar)\n            paragraphs = [p.strip() for p in re.split(r'\\n\\s*\\n', text) if p.strip()]\n            units = paragraphs\n        else:\n            # Split by sentences\n            sentences = sent_tokenize(text)\n            units = sentences\n        \n        # Create chunks with overlap\n        chunks = []\n        for i in range(0, len(units), chunk_size - chunk_overlap):\n            chunk = ' '.join(units[i:i + chunk_size])\n            if chunk:  # Only add non-empty chunks\n                chunks.append(chunk)\n        \n        return chunks\n    \n    def compute_embeddings(self, chunks: List[str]) -> np.ndarray:\n        \"\"\"\n        Compute embeddings for a list of text chunks\n        \n        Args:\n            chunks: List of text chunks\n            \n        Returns:\n            numpy array of embeddings\n        \"\"\"\n        return self.embedding_model.encode(chunks)\n    \n    def process_pdf(self, pdf_path: str, chunk_size: int = 3, \n                   chunk_overlap: int = 1, by_paragraph: bool = True) -> Tuple[List[str], np.ndarray]:\n        \"\"\"\n        Process a single PDF file: extract text, chunk it, and compute embeddings\n        \n        Args:\n            pdf_path: Path to the PDF file\n            chunk_size: Number of paragraphs/sentences per chunk\n            chunk_overlap: Number of paragraphs/sentences to overlap between chunks\n            by_paragraph: If True, chunk by paragraphs; otherwise by sentences\n            \n        Returns:\n            Tuple of (chunks, embeddings)\n        \"\"\"\n        logger.info(f\"Processing PDF: {os.path.basename(pdf_path)}\")\n        text = self.extract_text_from_pdf(pdf_path)\n        \n        if not text:\n            logger.warning(f\"No text extracted from {pdf_path}\")\n            return [], np.array([])\n        \n        chunks = self.chunk_text(text, chunk_size, chunk_overlap, by_paragraph)\n        \n        if not chunks:\n            logger.warning(f\"No chunks created from {pdf_path}\")\n            return [], np.array([])\n        \n        embeddings = self.compute_embeddings(chunks)\n        \n        logger.info(f\"Created {len(chunks)} chunks from {pdf_path}\")\n        return chunks, embeddings\n\nclass ChromaDBHandler:\n    def __init__(self, persist_directory: str = \"./chroma_db\"):\n        \"\"\"\n        Initialize the ChromaDB handler\n        \n        Args:\n            persist_directory: Directory to persist the ChromaDB\n        \"\"\"\n        self.client = chromadb.PersistentClient(path=persist_directory)\n        logger.info(f\"Initialized ChromaDB with persist directory: {persist_directory}\")\n    \n    def create_collection(self, collection_name: str) -> Any:\n        \"\"\"\n        Create or get a ChromaDB collection\n        \n        Args:\n            collection_name: Name of the collection\n            \n        Returns:\n            ChromaDB collection\n        \"\"\"\n        try:\n            # First try to get existing collection\n            collection = self.client.get_collection(name=collection_name)\n            logger.info(f\"Using existing collection: {collection_name}\")\n        except:\n            # If it doesn't exist, create a new one\n            collection = self.client.create_collection(name=collection_name)\n            logger.info(f\"Created new collection: {collection_name}\")\n        \n        return collection\n    \n    def add_to_collection(self, collection: Any, chunks: List[str], \n                         embeddings: np.ndarray, metadata: List[Dict[str, Any]], \n                         ids: List[str]) -> None:\n        \"\"\"\n        Add chunks and their embeddings to a ChromaDB collection\n        \n        Args:\n            collection: ChromaDB collection\n            chunks: List of text chunks\n            embeddings: numpy array of embeddings\n            metadata: List of metadata dictionaries for each chunk\n            ids: List of unique IDs for each chunk\n        \"\"\"\n        if not chunks:\n            logger.warning(\"No chunks to add to collection\")\n            return\n        \n        # ChromaDB expects embeddings as a list of lists\n        embeddings_list = embeddings.tolist()\n        \n        # Add chunks to collection in batches to prevent memory issues\n        batch_size = 100\n        for i in range(0, len(chunks), batch_size):\n            end_idx = min(i + batch_size, len(chunks))\n            \n            collection.add(\n                documents=chunks[i:end_idx],\n                embeddings=embeddings_list[i:end_idx],\n                metadatas=metadata[i:end_idx],\n                ids=ids[i:end_idx]\n            )\n        \n        logger.info(f\"Added {len(chunks)} chunks to collection\")\n\n# Configuration\nKAGGLE_INPUT_DIR = \"../input/dataset-of-pdf-files/Pdf\"  # Path to dataset in Kaggle\nCHROMA_DB_DIR = \"./chroma_db\"  # Where to store the vector DB\nCOLLECTION_NAME = \"pdf_collection\"  # Name of the ChromaDB collection\nEMBEDDING_MODEL = \"all-MiniLM-L6-v2\"  # SentenceTransformer model\nCHUNK_SIZE = 1  # Number of paragraphs per chunk\nCHUNK_OVERLAP = 1  # Overlap between chunks\nBY_PARAGRAPH = False  # Chunk by paragraph (True) or sentence (False)\nMAX_PDFS = 200  # Limit number of PDFs to process (set to None for all)\n\n# Initialize processors\npdf_processor = PDFProcessor(EMBEDDING_MODEL)\nchroma_handler = ChromaDBHandler(CHROMA_DB_DIR)\ncollection = chroma_handler.create_collection(COLLECTION_NAME)\n\n# Get list of PDF files\npdf_files = [f for f in os.listdir(KAGGLE_INPUT_DIR) if f.lower().endswith('.pdf')]\nlogger.info(f\"Found {len(pdf_files)} PDF files in {KAGGLE_INPUT_DIR}\")\n\n# Limit the number of PDFs to process if specified\nif MAX_PDFS:\n    pdf_files = pdf_files[:MAX_PDFS]\n    logger.info(f\"Processing {len(pdf_files)} PDFs (limited by MAX_PDFS setting)\")\n\n# Process metrics\nprocessed_count = 0\ntotal_chunks_count = 0\nfailed_count = 0\nstart_time = time.time()\n\n# Process each PDF file\nfor pdf_file in tqdm(pdf_files, desc=\"Processing PDFs\"):\n    pdf_path = os.path.join(KAGGLE_INPUT_DIR, pdf_file)\n    \n    # Generate a unique file ID\n    file_id = hashlib.md5(pdf_file.encode()).hexdigest()\n    \n    try:\n        # Process the PDF\n        chunks, embeddings = pdf_processor.process_pdf(\n            pdf_path, CHUNK_SIZE, CHUNK_OVERLAP, BY_PARAGRAPH\n        )\n        \n        if not chunks:\n            failed_count += 1\n            continue\n        \n        # Create metadata and IDs for each chunk\n        metadata = []\n        ids = []\n        \n        for i, chunk in enumerate(chunks):\n            chunk_id = f\"{file_id}_{i}\"\n            chunk_metadata = {\n                \"source\": pdf_file,\n                \"chunk_index\": i,\n                \"total_chunks\": len(chunks)\n            }\n            \n            metadata.append(chunk_metadata)\n            ids.append(chunk_id)\n        \n        # Add to ChromaDB collection\n        chroma_handler.add_to_collection(collection, chunks, embeddings, metadata, ids)\n        \n        processed_count += 1\n        total_chunks_count += len(chunks)\n        \n    except Exception as e:\n        logger.error(f\"Error processing {pdf_file}: {e}\")\n        failed_count += 1\n\n# Calculate processing time\nprocessing_time = time.time() - start_time\n\n# Display summary results\nprint(\"\\n--- Processing Summary ---\")\nprint(f\"Total PDFs found: {len(pdf_files)}\")\nprint(f\"Successfully processed: {processed_count}\")\nprint(f\"Failed: {failed_count}\")\nprint(f\"Total chunks created: {total_chunks_count}\")\nprint(f\"Processing time: {processing_time:.2f} seconds\")\nprint(f\"Vector database location: {CHROMA_DB_DIR}\")\n\n# Test the database with a simple query\nprint(\"\\n--- Testing Vector Database ---\")\nresults = collection.query(\n    query_texts=[\"What is the main topic of this document?\"],\n    n_results=3\n)\n\nprint(\"Query results:\")\nfor i, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0])):\n    print(f\"\\nResult {i+1}:\")\n    print(f\"Source: {metadata['source']}\")\n    print(f\"Chunk: {metadata['chunk_index']} of {metadata['total_chunks']}\")\n    print(f\"Content: {doc[:200]}...\")  # Show first 200 chars","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T10:36:41.043555Z","iopub.execute_input":"2025-04-26T10:36:41.043910Z","iopub.status.idle":"2025-04-26T10:37:15.225246Z","shell.execute_reply.started":"2025-04-26T10:36:41.043885Z","shell.execute_reply":"2025-04-26T10:37:15.224642Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Processing PDFs:   0%|          | 0/200 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5db605981144987a59a80de18002532"}},"metadata":{}},{"name":"stdout","text":"\n--- Processing Summary ---\nTotal PDFs found: 200\nSuccessfully processed: 0\nFailed: 200\nTotal chunks created: 0\nProcessing time: 22.47 seconds\nVector database location: ./chroma_db\n\n--- Testing Vector Database ---\n","output_type":"stream"},{"name":"stderr","text":"/root/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|██████████| 79.3M/79.3M [00:07<00:00, 11.8MiB/s]\n","output_type":"stream"},{"name":"stdout","text":"Query results:\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"!pip install -q openai-whisper gradio chromadb sentence-transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T10:37:45.830236Z","iopub.execute_input":"2025-04-26T10:37:45.830867Z","iopub.status.idle":"2025-04-26T10:37:55.227508Z","shell.execute_reply.started":"2025-04-26T10:37:45.830841Z","shell.execute_reply":"2025-04-26T10:37:55.226804Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.6/322.6 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import os\nimport numpy as np\nimport whisper\nimport gradio as gr\nimport torch\nimport chromadb\nfrom sentence_transformers import SentenceTransformer\nimport time\nimport logging\nfrom typing import Dict, List, Any\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# Path to the ChromaDB created in the previous step\nCHROMA_DB_DIR = \"./chroma_db\"\nCOLLECTION_NAME = \"pdf_collection\"\n\nclass ASRProcessor:\n    def __init__(self, model_size=\"base\"):\n        \"\"\"\n        Initialize the ASR processor with Whisper model\n        \n        Args:\n            model_size: Size of the Whisper model to use ('tiny', 'base', 'small', 'medium', 'large')\n        \"\"\"\n        logger.info(f\"Loading Whisper model: {model_size}\")\n        self.model = whisper.load_model(model_size)\n        logger.info(\"Whisper model loaded successfully\")\n    \n    def transcribe_audio(self, audio_path: str) -> Dict[str, Any]:\n        \"\"\"\n        Transcribe audio file to text\n        \n        Args:\n            audio_path: Path to the audio file\n            \n        Returns:\n            Dictionary with transcription results\n        \"\"\"\n        logger.info(f\"Transcribing audio file: {audio_path}\")\n        start_time = time.time()\n        \n        # Transcribe audio\n        result = self.model.transcribe(audio_path)\n        \n        processing_time = time.time() - start_time\n        logger.info(f\"Transcription completed in {processing_time:.2f} seconds\")\n        \n        return result\n\nclass RAGQueryProcessor:\n    def __init__(self, db_path: str = CHROMA_DB_DIR, collection_name: str = COLLECTION_NAME):\n        \"\"\"\n        Initialize the RAG query processor\n        \n        Args:\n            db_path: Path to the ChromaDB\n            collection_name: Name of the collection to query\n        \"\"\"\n        # Initialize embedding model\n        self.embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n        \n        # Connect to ChromaDB\n        self.client = chromadb.PersistentClient(path=db_path)\n        try:\n            self.collection = self.client.get_collection(name=collection_name)\n            logger.info(f\"Connected to existing collection: {collection_name}\")\n        except:\n            logger.error(f\"Collection {collection_name} not found in {db_path}\")\n            raise ValueError(f\"Collection {collection_name} not found. Please run the PDF ingestion step first.\")\n    \n    def query(self, query_text: str, n_results: int = 5) -> Dict[str, Any]:\n        \"\"\"\n        Process a query using RAG\n        \n        Args:\n            query_text: The query text\n            n_results: Number of results to retrieve\n            \n        Returns:\n            Dictionary with query results\n        \"\"\"\n        logger.info(f\"Processing query: {query_text}\")\n        \n        # Query the collection\n        results = self.collection.query(\n            query_texts=[query_text],\n            n_results=n_results\n        )\n        \n        # Format results\n        formatted_results = []\n        for i, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0])):\n            formatted_results.append({\n                \"rank\": i + 1,\n                \"source\": metadata['source'],\n                \"chunk_index\": metadata['chunk_index'],\n                \"content\": doc\n            })\n        \n        return {\n            \"query\": query_text,\n            \"results\": formatted_results\n        }\n\nclass VoiceRAGSystem:\n    def __init__(self, asr_model_size: str = \"base\"):\n        \"\"\"\n        Initialize the Voice RAG system\n        \n        Args:\n            asr_model_size: Size of the Whisper ASR model\n        \"\"\"\n        self.asr_processor = ASRProcessor(model_size=asr_model_size)\n        self.rag_processor = RAGQueryProcessor()\n        logger.info(\"Voice RAG system initialized\")\n    \n    def process_audio_query(self, audio_path: str, n_results: int = 5) -> Dict[str, Any]:\n        \"\"\"\n        Process an audio query through the ASR and RAG pipeline\n        \n        Args:\n            audio_path: Path to the audio file\n            n_results: Number of results to retrieve\n            \n        Returns:\n            Dictionary with processing results\n        \"\"\"\n        # Transcribe audio to text\n        transcription = self.asr_processor.transcribe_audio(audio_path)\n        query_text = transcription[\"text\"]\n        \n        # Process the transcribed query through RAG\n        rag_results = self.rag_processor.query(query_text, n_results)\n        \n        # Combine results\n        return {\n            \"transcription\": query_text,\n            \"rag_results\": rag_results[\"results\"]\n        }\n\n# Function to handle Gradio interface\ndef process_audio(audio_file, num_results):\n    \"\"\"\n    Process audio file through the Voice RAG system\n    \n    Args:\n        audio_file: Path to the uploaded audio file\n        num_results: Number of results to retrieve\n        \n    Returns:\n        Formatted results for display\n    \"\"\"\n    try:\n        # Initialize the Voice RAG system\n        # Using the small model for better performance while still having good accuracy\n        voice_rag = VoiceRAGSystem(asr_model_size=\"small\")\n        \n        # Process the audio query\n        results = voice_rag.process_audio_query(audio_file, n_results=num_results)\n        \n        # Format results for display\n        transcription = results[\"transcription\"]\n        \n        # Format RAG results\n        rag_results_formatted = \"\"\n        for i, result in enumerate(results[\"rag_results\"]):\n            rag_results_formatted += f\"**Result {i+1}:** Source: {result['source']}\\n\\n\"\n            rag_results_formatted += f\"{result['content'][:500]}...\\n\\n\"\n            rag_results_formatted += \"---\\n\\n\"\n        \n        return transcription, rag_results_formatted\n    \n    except Exception as e:\n        logger.error(f\"Error processing audio: {e}\")\n        return f\"Error: {str(e)}\", \"Failed to process query\"\n\n# Create Gradio interface\ndef create_interface():\n    \"\"\"\n    Create a Gradio interface for the Voice RAG system\n    \n    Returns:\n        Gradio interface\n    \"\"\"\n    # Define the interface\n    interface = gr.Interface(\n        fn=process_audio,\n        inputs=[\n            gr.Audio(type=\"filepath\", label=\"Record or Upload Audio\"),\n            gr.Slider(minimum=1, maximum=10, value=5, step=1, label=\"Number of Results\")\n        ],\n        outputs=[\n            gr.Textbox(label=\"Transcribed Query\"),\n            gr.Markdown(label=\"Retrieval Results\")\n        ],\n        title=\"Voice-Driven RAG System\",\n        description=\"Speak or upload an audio file containing your question about the documents. The system will transcribe your speech and retrieve relevant information from the PDF database.\",\n        examples=[\n            [\"example_query.mp3\", 5]\n        ]\n    )\n    \n    return interface\n\n# Download Whisper model when the notebook runs\ndef download_whisper_model(model_size=\"small\"):\n    \"\"\"\n    Download the Whisper model proactively\n    \n    Args:\n        model_size: Size of the model to download\n    \"\"\"\n    logger.info(f\"Pre-downloading Whisper {model_size} model...\")\n    whisper.load_model(model_size)\n    logger.info(\"Model download complete\")\n\n# Main execution\nif __name__ == \"__main__\":\n    # Pre-download the model\n    download_whisper_model(\"small\")\n    \n    # Create and launch the interface\n    demo = create_interface()\n    demo.launch(share=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T10:41:05.695041Z","iopub.execute_input":"2025-04-26T10:41:05.695727Z","iopub.status.idle":"2025-04-26T10:41:12.438456Z","shell.execute_reply.started":"2025-04-26T10:41:05.695695Z","shell.execute_reply":"2025-04-26T10:41:12.437902Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(fp, map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7861\n* Running on public URL: https://f8047e259d5979355d.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://f8047e259d5979355d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(fp, map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"Using existing dataset file at: .gradio/flagged/dataset1.csv\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Install required packages\n!pip install -q transformers==4.35.2 accelerate==0.25.0 bitsandbytes==0.41.1 chromadb==0.4.18 sentence-transformers==2.2.2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T10:42:20.254039Z","iopub.execute_input":"2025-04-26T10:42:20.254318Z","iopub.status.idle":"2025-04-26T10:42:37.236781Z","shell.execute_reply.started":"2025-04-26T10:42:20.254297Z","shell.execute_reply":"2025-04-26T10:42:37.235784Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# ChromaDB Diagnostics and Fix\n# Run this code to diagnose and fix your ChromaDB connection issues\n\n# Install required packages\n!pip install -q chromadb sentence-transformers\n\nimport os\nimport chromadb\nimport logging\nfrom sentence_transformers import SentenceTransformer\nimport time\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# Common ChromaDB paths to check\npossible_paths = [\n    \"./chroma_db\",\n    \"./kaggle/working/chroma_db\",\n    \"/kaggle/working/chroma_db\",\n    \"../chroma_db\",\n    \"/kaggle/input/dataset-of-pdf-files/chroma_db\",  # In case you exported it\n]\n\n# Possible collection names\npossible_collections = [\n    \"pdf_collection\",\n    \"document_collection\",\n    \"vector_store\"\n]\n\ndef find_chroma_db():\n    \"\"\"Search for ChromaDB in all possible locations\"\"\"\n    print(\"\\n=== Searching for ChromaDB ===\")\n    \n    for path in possible_paths:\n        if os.path.exists(path):\n            print(f\"✅ Found directory: {path}\")\n            try:\n                client = chromadb.PersistentClient(path=path)\n                collections = client.list_collections()\n                if collections:\n                    print(f\"✅ Connected to ChromaDB at {path}\")\n                    print(f\"Found {len(collections)} collections:\")\n                    for coll in collections:\n                        print(f\"  - {coll.name} (count: {coll.count()})\")\n                    return path, collections\n                else:\n                    print(f\"❌ No collections found in {path}\")\n            except Exception as e:\n                print(f\"❌ Error connecting to ChromaDB at {path}: {e}\")\n        else:\n            print(f\"❌ Directory not found: {path}\")\n    \n    return None, None\n\ndef test_retrieval(db_path, collection_name):\n    \"\"\"Test retrieval from a specific collection\"\"\"\n    print(f\"\\n=== Testing Retrieval from {collection_name} ===\")\n    \n    try:\n        # Connect to ChromaDB\n        client = chromadb.PersistentClient(path=db_path)\n        collection = client.get_collection(name=collection_name)\n        \n        # Get collection stats\n        count = collection.count()\n        print(f\"Collection '{collection_name}' contains {count} documents\")\n        \n        if count == 0:\n            print(\"❌ Collection is empty - no documents to retrieve\")\n            return False\n        \n        # Try a generic query\n        print(\"Testing retrieval with generic query...\")\n        results = collection.query(\n            query_texts=[\"document information\"],\n            n_results=5\n        )\n        \n        if results['documents'] and len(results['documents'][0]) > 0:\n            print(f\"✅ Successfully retrieved {len(results['documents'][0])} documents\")\n            print(\"\\nSample document content:\")\n            print(f\"Source: {results['metadatas'][0][0]['source']}\")\n            print(f\"Content: {results['documents'][0][0][:200]}...\")\n            return True\n        else:\n            print(\"❌ No documents retrieved for generic query\")\n            return False\n    \n    except Exception as e:\n        print(f\"❌ Error testing retrieval: {e}\")\n        return False\n\ndef rebuild_index(db_path):\n    \"\"\"Rebuild the index if needed with basic documents from scratch\"\"\"\n    print(\"\\n=== Rebuilding Index ===\")\n    \n    try:\n        # Initialize ChromaDB\n        client = chromadb.PersistentClient(path=db_path)\n        \n        # Check if pdf_collection already exists\n        collections = client.list_collections()\n        collection_names = [c.name for c in collections]\n        \n        if \"pdf_collection\" in collection_names:\n            print(\"Removing existing pdf_collection...\")\n            client.delete_collection(\"pdf_collection\")\n        \n        # Create new collection\n        collection = client.create_collection(name=\"pdf_collection\")\n        print(\"✅ Created new pdf_collection\")\n        \n        # Add some sample documents\n        print(\"Adding sample documents...\")\n        \n        # Sample dummy documents to initialize with\n        docs = [\n            \"This is a sample document about artificial intelligence and machine learning.\",\n            \"PDF files contain structured information in a portable document format.\",\n            \"Natural language processing helps computers understand human language.\",\n            \"Document retrieval systems help find relevant information quickly.\",\n            \"Vector databases store embeddings for semantic search applications.\"\n        ]\n        \n        # Generate IDs and metadata\n        ids = [f\"doc_{i}\" for i in range(len(docs))]\n        metadatas = [{\"source\": f\"sample_doc_{i}.pdf\", \"chunk_index\": 0, \"total_chunks\": 1} for i in range(len(docs))]\n        \n        # Initialize embedding model\n        embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n        \n        # Compute embeddings\n        embeddings = embedding_model.encode(docs).tolist()\n        \n        # Add documents to collection\n        collection.add(\n            documents=docs,\n            embeddings=embeddings,\n            metadatas=metadatas,\n            ids=ids\n        )\n        \n        print(f\"✅ Added {len(docs)} sample documents to collection\")\n        \n        # Test retrieval\n        test_retrieval(db_path, \"pdf_collection\")\n        \n        return True\n    \n    except Exception as e:\n        print(f\"❌ Error rebuilding index: {e}\")\n        return False\n\n# Main execution\nprint(\"=== ChromaDB Diagnostics ===\")\nprint(f\"Current working directory: {os.getcwd()}\")\n\n# Find ChromaDB\ndb_path, collections = find_chroma_db()\n\nif db_path and collections:\n    # Try to test retrieval on each collection\n    retrieval_success = False\n    for collection in collections:\n        if test_retrieval(db_path, collection.name):\n            retrieval_success = True\n            print(f\"\\n✅ Successfully tested retrieval with collection: {collection.name}\")\n            print(f\"Use this collection name: {collection.name}\")\n            print(f\"And this DB path: {db_path}\")\n            break\n    \n    if not retrieval_success:\n        print(\"\\n❌ Could not retrieve documents from any existing collection\")\n        print(\"Attempting to rebuild the index...\")\n        rebuild_index(db_path)\nelse:\n    print(\"\\n❌ Could not find a valid ChromaDB directory\")\n    print(\"Creating a new ChromaDB with sample documents...\")\n    \n    # Create a new directory for ChromaDB\n    new_db_path = \"./chroma_db\"\n    os.makedirs(new_db_path, exist_ok=True)\n    \n    # Rebuild index\n    rebuild_index(new_db_path)\n    print(f\"\\nCreated new ChromaDB at: {new_db_path}\")\n    print(\"Use the collection name: pdf_collection\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T10:42:39.911013Z","iopub.execute_input":"2025-04-26T10:42:39.911776Z","iopub.status.idle":"2025-04-26T10:42:46.169409Z","shell.execute_reply.started":"2025-04-26T10:42:39.911739Z","shell.execute_reply":"2025-04-26T10:42:46.168745Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"=== ChromaDB Diagnostics ===\nCurrent working directory: /kaggle/working\n\n=== Searching for ChromaDB ===\n✅ Found directory: ./chroma_db\n✅ Connected to ChromaDB at ./chroma_db\nFound 1 collections:\n  - pdf_collection (count: 0)\n\n=== Testing Retrieval from pdf_collection ===\nCollection 'pdf_collection' contains 0 documents\n❌ Collection is empty - no documents to retrieve\n\n❌ Could not retrieve documents from any existing collection\nAttempting to rebuild the index...\n\n=== Rebuilding Index ===\nRemoving existing pdf_collection...\n✅ Created new pdf_collection\nAdding sample documents...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75b69c7ef7ba49f4b587af9478ec710c"}},"metadata":{}},{"name":"stdout","text":"✅ Added 5 sample documents to collection\n\n=== Testing Retrieval from pdf_collection ===\nCollection 'pdf_collection' contains 5 documents\nTesting retrieval with generic query...\n✅ Successfully retrieved 5 documents\n\nSample document content:\nSource: sample_doc_1.pdf\nContent: PDF files contain structured information in a portable document format....\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# RAG System with Llama 2 via Replicate API\n# This implementation uses Replicate to access Llama 2\n\n# Install required packages\n!pip install -q gradio chromadb sentence-transformers openai-whisper PyMuPDF replicate\n\nimport os\nimport numpy as np\nimport whisper\nimport gradio as gr\nimport time\nimport logging\nfrom typing import Dict, List, Any\nimport chromadb\nfrom sentence_transformers import SentenceTransformer\nimport fitz  # PyMuPDF\nimport re\nimport hashlib\nimport replicate\nimport nltk\nfrom nltk.tokenize import sent_tokenize\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# Path configurations\nPDF_DIR = \"../input/dataset-of-pdf-files/Pdf\"\nCHROMA_DB_DIR = \"./chroma_db\"\nCOLLECTION_NAME = \"pdf_collection\"\n\n# Replicate API token - replace with your own\n# You'll need to sign up at https://replicate.com/ and get an API token\nos.environ[\"REPLICATE_API_TOKEN\"] = \"r8_9EHL1giipzTCFKnRHLm5Ht72PSZaTYD3YEymn\"  # You'll need to replace this\n\n# Download NLTK resources\ntry:\n    nltk.data.find('tokenizers/punkt')\nexcept LookupError:\n    nltk.download('punkt')\n\n# ====== PDF Processing Component ======\n# (Same as before, keeping code for completeness)\n\nclass PDFProcessor:\n    def __init__(self, embedding_model_name: str = \"all-MiniLM-L6-v2\"):\n        \"\"\"\n        Initialize the PDF processor with specified embedding model\n        \n        Args:\n            embedding_model_name: The name of the SentenceTransformer model to use\n        \"\"\"\n        self.embedding_model = SentenceTransformer(embedding_model_name)\n        logger.info(f\"Initialized embedding model: {embedding_model_name}\")\n    \n    def extract_text_from_pdf(self, pdf_path: str) -> str:\n        \"\"\"\n        Extract all text from a PDF file\n        \n        Args:\n            pdf_path: Path to the PDF file\n            \n        Returns:\n            Extracted text as a single string\n        \"\"\"\n        try:\n            doc = fitz.open(pdf_path)\n            text = \"\"\n            \n            for page_num in range(len(doc)):\n                page = doc.load_page(page_num)\n                text += page.get_text()\n                \n            doc.close()\n            return text\n        except Exception as e:\n            logger.error(f\"Error extracting text from {pdf_path}: {e}\")\n            return \"\"\n    \n    def chunk_text(self, text: str, chunk_size: int = 3, \n                  chunk_overlap: int = 1, by_paragraph: bool = True) -> List[str]:\n        \"\"\"\n        Split text into chunks by paragraphs or sentences\n        \n        Args:\n            text: The text to chunk\n            chunk_size: Number of paragraphs/sentences per chunk\n            chunk_overlap: Number of paragraphs/sentences to overlap between chunks\n            by_paragraph: If True, chunk by paragraphs; otherwise by sentences\n            \n        Returns:\n            List of text chunks\n        \"\"\"\n        # Clean and normalize text\n        text = re.sub(r'\\s+', ' ', text).strip()\n        \n        if by_paragraph:\n            # Split by paragraphs (defined by double newlines or similar)\n            paragraphs = [p.strip() for p in re.split(r'\\n\\s*\\n', text) if p.strip()]\n            units = paragraphs\n        else:\n            # Split by sentences\n            sentences = sent_tokenize(text)\n            units = sentences\n        \n        # Create chunks with overlap\n        chunks = []\n        for i in range(0, len(units), max(1, chunk_size - chunk_overlap)):\n            chunk = ' '.join(units[i:i + chunk_size])\n            if chunk:  # Only add non-empty chunks\n                chunks.append(chunk)\n        \n        return chunks\n    \n    def compute_embeddings(self, chunks: List[str]) -> np.ndarray:\n        \"\"\"\n        Compute embeddings for a list of text chunks\n        \n        Args:\n            chunks: List of text chunks\n            \n        Returns:\n            numpy array of embeddings\n        \"\"\"\n        return self.embedding_model.encode(chunks)\n    \n    def process_pdf(self, pdf_path: str, chunk_size: int = 3, \n                   chunk_overlap: int = 1, by_paragraph: bool = True) -> Dict[str, Any]:\n        \"\"\"\n        Process a single PDF file: extract text, chunk it, and compute embeddings\n        \n        Args:\n            pdf_path: Path to the PDF file\n            chunk_size: Number of paragraphs/sentences per chunk\n            chunk_overlap: Number of paragraphs/sentences to overlap between chunks\n            by_paragraph: If True, chunk by paragraphs; otherwise by sentences\n            \n        Returns:\n            Dictionary with chunks, embeddings, and metadata\n        \"\"\"\n        logger.info(f\"Processing PDF: {os.path.basename(pdf_path)}\")\n        text = self.extract_text_from_pdf(pdf_path)\n        \n        if not text:\n            logger.warning(f\"No text extracted from {pdf_path}\")\n            return {\"chunks\": [], \"embeddings\": np.array([]), \"metadata\": []}\n        \n        chunks = self.chunk_text(text, chunk_size, chunk_overlap, by_paragraph)\n        \n        if not chunks:\n            logger.warning(f\"No chunks created from {pdf_path}\")\n            return {\"chunks\": [], \"embeddings\": np.array([]), \"metadata\": []}\n        \n        embeddings = self.compute_embeddings(chunks)\n        \n        # Create metadata\n        file_id = hashlib.md5(os.path.basename(pdf_path).encode()).hexdigest()\n        metadata = []\n        for i in range(len(chunks)):\n            metadata.append({\n                \"source\": os.path.basename(pdf_path),\n                \"chunk_index\": i,\n                \"total_chunks\": len(chunks)\n            })\n        \n        logger.info(f\"Created {len(chunks)} chunks from {pdf_path}\")\n        return {\n            \"chunks\": chunks,\n            \"embeddings\": embeddings,\n            \"metadata\": metadata,\n            \"ids\": [f\"{file_id}_{i}\" for i in range(len(chunks))]\n        }\n\n# ====== ChromaDB Handler Component ======\n\nclass ChromaDBHandler:\n    def __init__(self, persist_directory: str = CHROMA_DB_DIR):\n        \"\"\"\n        Initialize the ChromaDB handler\n        \n        Args:\n            persist_directory: Directory to persist the ChromaDB\n        \"\"\"\n        # Create directory if it doesn't exist\n        os.makedirs(persist_directory, exist_ok=True)\n        \n        self.client = chromadb.PersistentClient(path=persist_directory)\n        logger.info(f\"Initialized ChromaDB with persist directory: {persist_directory}\")\n    \n    def create_collection(self, collection_name: str) -> Any:\n        \"\"\"\n        Create or get a ChromaDB collection\n        \n        Args:\n            collection_name: Name of the collection\n            \n        Returns:\n            ChromaDB collection\n        \"\"\"\n        try:\n            # First try to get existing collection\n            collection = self.client.get_collection(name=collection_name)\n            logger.info(f\"Using existing collection: {collection_name}\")\n        except:\n            # If it doesn't exist, create a new one\n            collection = self.client.create_collection(name=collection_name)\n            logger.info(f\"Created new collection: {collection_name}\")\n        \n        return collection\n    \n    def add_to_collection(self, collection: Any, chunks: List[str], \n                         embeddings: np.ndarray, metadata: List[Dict[str, Any]], \n                         ids: List[str]) -> None:\n        \"\"\"\n        Add chunks and their embeddings to a ChromaDB collection\n        \n        Args:\n            collection: ChromaDB collection\n            chunks: List of text chunks\n            embeddings: numpy array of embeddings\n            metadata: List of metadata dictionaries for each chunk\n            ids: List of unique IDs for each chunk\n        \"\"\"\n        if not chunks:\n            logger.warning(\"No chunks to add to collection\")\n            return\n        \n        # ChromaDB expects embeddings as a list of lists\n        embeddings_list = embeddings.tolist()\n        \n        # Add chunks to collection in batches to prevent memory issues\n        batch_size = 100\n        for i in range(0, len(chunks), batch_size):\n            end_idx = min(i + batch_size, len(chunks))\n            \n            collection.add(\n                documents=chunks[i:end_idx],\n                embeddings=embeddings_list[i:end_idx],\n                metadatas=metadata[i:end_idx],\n                ids=ids[i:end_idx]\n            )\n        \n        logger.info(f\"Added {len(chunks)} chunks to collection\")\n\n# ====== ASR Component ======\n\nclass ASRProcessor:\n    def __init__(self, model_size=\"small\"):\n        \"\"\"\n        Initialize the ASR processor with Whisper model\n        \n        Args:\n            model_size: Size of the Whisper model to use ('tiny', 'base', 'small', 'medium', 'large')\n        \"\"\"\n        logger.info(f\"Loading Whisper model: {model_size}\")\n        self.model = whisper.load_model(model_size)\n        logger.info(\"Whisper model loaded successfully\")\n    \n    def transcribe_audio(self, audio_path: str) -> Dict[str, Any]:\n        \"\"\"\n        Transcribe audio file to text\n        \n        Args:\n            audio_path: Path to the audio file\n            \n        Returns:\n            Dictionary with transcription results\n        \"\"\"\n        logger.info(f\"Transcribing audio file: {audio_path}\")\n        start_time = time.time()\n        \n        # Transcribe audio\n        result = self.model.transcribe(audio_path)\n        \n        processing_time = time.time() - start_time\n        logger.info(f\"Transcription completed in {processing_time:.2f} seconds\")\n        \n        return result\n\n# ====== RAG Retriever Component ======\n\nclass RAGRetriever:\n    def __init__(self, db_path: str = CHROMA_DB_DIR, collection_name: str = COLLECTION_NAME):\n        \"\"\"\n        Initialize the RAG retriever\n        \n        Args:\n            db_path: Path to the ChromaDB\n            collection_name: Name of the collection to query\n        \"\"\"\n        # Initialize embedding model\n        self.embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n        \n        # Connect to ChromaDB\n        try:\n            self.client = chromadb.PersistentClient(path=db_path)\n            \n            # Check if collection exists\n            collections = self.client.list_collections()\n            collection_names = [c.name for c in collections]\n            \n            if collection_name in collection_names:\n                self.collection = self.client.get_collection(name=collection_name)\n                logger.info(f\"Connected to existing collection: {collection_name}\")\n            else:\n                # Collection doesn't exist, create it\n                logger.info(f\"Collection {collection_name} not found. Creating new collection.\")\n                self.collection = self.client.create_collection(name=collection_name)\n                \n                # Process some PDFs to populate the collection\n                self._populate_collection()\n        except Exception as e:\n            logger.error(f\"Error connecting to ChromaDB: {e}\")\n            # Create a new ChromaDB\n            os.makedirs(db_path, exist_ok=True)\n            self.client = chromadb.PersistentClient(path=db_path)\n            self.collection = self.client.create_collection(name=collection_name)\n            \n            # Process some PDFs to populate the collection\n            self._populate_collection()\n    \n    def _populate_collection(self, max_pdfs: int = 10):\n        \"\"\"\n        Populate the collection with some PDFs\n        \n        Args:\n            max_pdfs: Maximum number of PDFs to process\n        \"\"\"\n        logger.info(\"Populating collection with PDFs...\")\n        \n        # Initialize PDF processor\n        pdf_processor = PDFProcessor()\n        \n        # Get list of PDF files\n        if os.path.exists(PDF_DIR):\n            pdf_files = [f for f in os.listdir(PDF_DIR) if f.lower().endswith('.pdf')]\n            logger.info(f\"Found {len(pdf_files)} PDF files in {PDF_DIR}\")\n            \n            # Limit the number of PDFs to process\n            pdf_files = pdf_files[:max_pdfs]\n            \n            # Process each PDF\n            for pdf_file in pdf_files:\n                pdf_path = os.path.join(PDF_DIR, pdf_file)\n                \n                # Process the PDF\n                result = pdf_processor.process_pdf(pdf_path, chunk_size=1, by_paragraph=False)\n                \n                if result[\"chunks\"]:\n                    # Add to collection\n                    self.collection.add(\n                        documents=result[\"chunks\"],\n                        embeddings=result[\"embeddings\"].tolist(),\n                        metadatas=result[\"metadata\"],\n                        ids=result[\"ids\"]\n                    )\n                    \n                    logger.info(f\"Added {len(result['chunks'])} chunks from {pdf_file}\")\n        else:\n            logger.warning(f\"PDF directory not found: {PDF_DIR}\")\n            \n            # Create some dummy documents if no PDFs are available\n            dummy_docs = [\n                \"This is a sample document about PDFs and document processing.\",\n                \"PDFs contain structured information that can be extracted and analyzed.\",\n                \"RAG systems combine retrieval with generation to provide accurate answers.\",\n                \"Vector databases store embeddings for semantic search applications.\",\n                \"Natural language processing helps computers understand human language.\"\n            ]\n            \n            # Generate embeddings\n            embeddings = pdf_processor.compute_embeddings(dummy_docs)\n            \n            # Create metadata\n            metadata = []\n            ids = []\n            for i, _ in enumerate(dummy_docs):\n                metadata.append({\n                    \"source\": f\"sample_doc_{i}.pdf\",\n                    \"chunk_index\": 0,\n                    \"total_chunks\": 1\n                })\n                ids.append(f\"dummy_{i}\")\n            \n            # Add to collection\n            self.collection.add(\n                documents=dummy_docs,\n                embeddings=embeddings.tolist(),\n                metadatas=metadata,\n                ids=ids\n            )\n            \n            logger.info(f\"Added {len(dummy_docs)} dummy documents to collection\")\n    \n    def retrieve(self, query_text: str, n_results: int = 5) -> Dict[str, Any]:\n        \"\"\"\n        Retrieve relevant chunks for a query\n        \n        Args:\n            query_text: The query text\n            n_results: Number of results to retrieve\n            \n        Returns:\n            Dictionary with query results\n        \"\"\"\n        logger.info(f\"Retrieving documents for query: {query_text}\")\n        \n        # Check if collection is empty\n        if self.collection.count() == 0:\n            logger.warning(\"Collection is empty. Populating with sample documents.\")\n            self._populate_collection()\n        \n        # Query the collection\n        results = self.collection.query(\n            query_texts=[query_text],\n            n_results=n_results\n        )\n        \n        # Extract documents and metadata\n        documents = results['documents'][0] if results['documents'] and results['documents'][0] else []\n        metadatas = results['metadatas'][0] if results['metadatas'] and results['metadatas'][0] else []\n        \n        # Format results\n        formatted_results = []\n        for i, (doc, metadata) in enumerate(zip(documents, metadatas)):\n            formatted_results.append({\n                \"rank\": i + 1,\n                \"source\": metadata['source'],\n                \"chunk_index\": metadata['chunk_index'],\n                \"content\": doc\n            })\n        \n        return {\n            \"query\": query_text,\n            \"results\": formatted_results\n        }\n    \n    def get_context_string(self, results: Dict[str, Any]) -> str:\n        \"\"\"\n        Create a context string from retrieval results\n        \n        Args:\n            results: The retrieval results\n            \n        Returns:\n            Formatted context string\n        \"\"\"\n        context = \"\"\n        for i, result in enumerate(results[\"results\"]):\n            context += f\"Document {i+1} (Source: {result['source']}):\\n{result['content']}\\n\\n\"\n        \n        return context.strip()\n\n# ====== Llama 2 Generator using Replicate ======\n\nclass Llama2Generator:\n    def __init__(self):\n        \"\"\"\n        Initialize the Llama 2 generator using Replicate API\n        \"\"\"\n        logger.info(\"Initializing Llama 2 generator using Replicate API\")\n        \n        # Check if API token is set\n        if \"REPLICATE_API_TOKEN\" not in os.environ or not os.environ[\"REPLICATE_API_TOKEN\"] or os.environ[\"REPLICATE_API_TOKEN\"] == \"YOUR_REPLICATE_API_TOKEN\":\n            logger.warning(\"REPLICATE_API_TOKEN not set or using default value.\")\n            logger.warning(\"Please set your Replicate API token to use Llama 2.\")\n            self.api_available = False\n        else:\n            self.api_available = True\n        \n        # Define the model\n        self.model = \"meta/llama-2-7b-chat:f1d50bb24186c52daae319ca8366e53debdaa9e0ae7ff976e918df752732ccc4\"\n        \n        logger.info(\"Llama 2 generator initialized successfully\")\n    \n    def generate_answer(self, query: str, context: str) -> str:\n        \"\"\"\n        Generate an answer based on query and context using Llama 2 via Replicate\n        \n        Args:\n            query: The query text\n            context: The retrieved context\n            \n        Returns:\n            Generated answer\n        \"\"\"\n        logger.info(\"Generating answer with Llama 2 via Replicate\")\n        \n        if not self.api_available:\n            return (\"To use Llama 2 for generating answers, please set your Replicate API token.\\n\\n\"\n                   \"1. Sign up at https://replicate.com/\\n\"\n                   \"2. Get your API token from your account settings\\n\"\n                   \"3. Set it in the code with: os.environ[\\\"REPLICATE_API_TOKEN\\\"] = \\\"your_token\\\"\")\n        \n        # Create prompt with context and query\n        prompt = self._create_prompt(query, context)\n        \n        start_time = time.time()\n        \n        try:\n            # Generate response with Replicate\n            output = replicate.run(\n                self.model,\n                input={\n                    \"prompt\": prompt,\n                    \"temperature\": 0.7,\n                    \"top_p\": 0.9,\n                    \"max_new_tokens\": 500,\n                    \"repetition_penalty\": 1.1\n                }\n            )\n            \n            # Collect the streaming output\n            full_response = \"\"\n            for item in output:\n                full_response += item\n            \n            processing_time = time.time() - start_time\n            logger.info(f\"Answer generation completed in {processing_time:.2f} seconds\")\n            \n            return full_response\n            \n        except Exception as e:\n            logger.error(f\"Error generating answer with Replicate: {e}\")\n            return f\"Error generating answer: {str(e)}\"\n    \n    def _create_prompt(self, query: str, context: str) -> str:\n        \"\"\"\n        Create a prompt for Llama 2\n        \n        Args:\n            query: The query text\n            context: The retrieved context\n            \n        Returns:\n            Formatted prompt\n        \"\"\"\n        # Llama 2 chat format requires a specific template\n        system_prompt = \"\"\"You are a helpful AI assistant that answers questions based on the provided document contexts. \nYour task is to provide accurate, concise answers based solely on the information in the documents.\nIf the answer cannot be found in the documents, acknowledge that you don't have enough information.\"\"\"\n        \n        # Format the prompt according to Llama 2 chat template\n        prompt = f\"<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\nI need information from the following documents:\\n\\n{context}\\n\\nBased on these documents, please answer the following question: {query} [/INST]\"\n        \n        return prompt\n\n# ====== Document Summarization Component ======\n\nclass DocumentSummarizer:\n    def __init__(self):\n        \"\"\"\n        Initialize the document summarizer using Replicate API\n        \"\"\"\n        logger.info(\"Initializing document summarizer using Replicate API\")\n        \n        # Check if API token is set\n        if \"REPLICATE_API_TOKEN\" not in os.environ or not os.environ[\"REPLICATE_API_TOKEN\"] or os.environ[\"REPLICATE_API_TOKEN\"] == \"YOUR_REPLICATE_API_TOKEN\":\n            logger.warning(\"REPLICATE_API_TOKEN not set or using default value.\")\n            logger.warning(\"Please set your Replicate API token to use the summarizer.\")\n            self.api_available = False\n        else:\n            self.api_available = True\n        \n        # Define the model (Using BART-large-CNN for summarization)\n        self.model = \"facebook/bart-large-cnn:c850aa8c6320ac07afbd3076afaccfb4c4bac0b4c30f4c4df3392de8f28adc7e\"\n        \n        logger.info(\"Document summarizer initialized successfully\")\n    \n    def summarize_text(self, text: str, max_length: int = 150, min_length: int = 40) -> str:\n        \"\"\"\n        Generate a summary of the provided text\n        \n        Args:\n            text: The text to summarize\n            max_length: Maximum length of the summary\n            min_length: Minimum length of the summary\n            \n        Returns:\n            Generated summary\n        \"\"\"\n        logger.info(f\"Generating summary with max_length={max_length}, min_length={min_length}\")\n        \n        if not self.api_available:\n            return (\"To use the summarization feature, please set your Replicate API token.\\n\\n\"\n                   \"1. Sign up at https://replicate.com/\\n\"\n                   \"2. Get your API token from your account settings\\n\"\n                   \"3. Set it in the code with: os.environ[\\\"REPLICATE_API_TOKEN\\\"] = \\\"your_token\\\"\")\n        \n        # Limit text length to prevent API errors\n        max_input_length = 1024  # Model limit\n        if len(text) > max_input_length:\n            text = text[:max_input_length]\n        \n        try:\n            # Generate summary with Replicate\n            output = replicate.run(\n                self.model,\n                input={\n                    \"inputs\": text,\n                    \"min_length\": min_length,\n                    \"max_length\": max_length,\n                    \"temperature\": 1.0,\n                    \"beam_size\": 4\n                }\n            )\n            \n            # Output is a single string\n            return output\n            \n        except Exception as e:\n            logger.error(f\"Error generating summary: {e}\")\n            return f\"Error generating summary: {str(e)}\"\n    \n    def summarize_documents(self, documents: List[Dict[str, Any]], max_length: int = 200) -> str:\n        \"\"\"\n        Summarize a list of retrieved documents\n        \n        Args:\n            documents: List of document dictionaries from retrieval\n            max_length: Maximum length of the summary\n            \n        Returns:\n            Generated summary of all documents\n        \"\"\"\n        # Combine document texts\n        combined_text = \"\"\n        for doc in documents:\n            combined_text += f\"{doc['content']}\\n\\n\"\n        \n        # Generate summary\n        return self.summarize_text(combined_text, max_length=max_length)\n\n# ====== RAG QA System ======\n\nclass RAGQASystem:\n    def __init__(self):\n        \"\"\"Initialize the RAG QA system with Llama 2 via Replicate\"\"\"\n        self.retriever = RAGRetriever()\n        self.generator = Llama2Generator()\n    \n    def process_query(self, query_text: str, n_results: int = 5) -> Dict[str, Any]:\n        \"\"\"\n        Process a query through the RAG QA pipeline\n        \n        Args:\n            query_text: The query text\n            n_results: Number of results to retrieve\n            \n        Returns:\n            Dictionary with processed results\n        \"\"\"\n        # Retrieve relevant documents\n        retrieval_results = self.retriever.retrieve(query_text, n_results)\n        \n        # If no results were retrieved, return early\n        if not retrieval_results[\"results\"]:\n            return {\n                \"query\": query_text,\n                \"retrieved_docs\": [],\n                \"context\": \"\",\n                \"answer\": \"No relevant documents found for your query.\"\n            }\n        \n        # Create context string from retrieved documents\n        context = self.retriever.get_context_string(retrieval_results)\n        \n        # Generate answer\n        try:\n            answer = self.generator.generate_answer(query_text, context)\n        except Exception as e:\n            logger.error(f\"Error generating answer: {e}\")\n            answer = f\"Error generating answer: {str(e)}\"\n        \n        # Return all results\n        return {\n            \"query\": query_text,\n            \"retrieved_docs\": retrieval_results[\"results\"],\n            \"context\": context,\n            \"answer\": answer\n        }\n\n# ====== Voice-Driven RAG System ======\n\nclass VoiceRAGSystem:\n    def __init__(self, asr_model_size: str = \"small\"):\n        \"\"\"\n        Initialize the Voice RAG system\n        \n        Args:\n            asr_model_size: Size of the Whisper ASR model\n        \"\"\"\n        self.asr_processor = ASRProcessor(model_size=asr_model_size)\n        self.rag_qa_system = RAGQASystem()\n        logger.info(\"Voice RAG system initialized\")\n    \n    def process_audio_query(self, audio_path: str, n_results: int = 5) -> Dict[str, Any]:\n        \"\"\"\n        Process an audio query through the ASR and RAG QA pipeline\n        \n        Args:\n            audio_path: Path to the audio file\n            n_results: Number of results to retrieve\n            \n        Returns:\n            Dictionary with processing results\n        \"\"\"\n        # Transcribe audio to text\n        transcription = self.asr_processor.transcribe_audio(audio_path)\n        query_text = transcription[\"text\"]\n        \n        # Process the transcribed query through RAG QA\n        qa_results = self.rag_qa_system.process_query(query_text, n_results)\n        \n        # Combine results\n        return {\n            \"transcription\": query_text,\n            \"retrieved_docs\": qa_results[\"retrieved_docs\"],\n            \"answer\": qa_results[\"answer\"]\n        }\n\n# ====== Gradio Interface Functions ======\n\n# Global system instance (to avoid reloading models for each query)\nvoice_rag_system = None\nsummarizer = None\n\ndef initialize_systems():\n    \"\"\"Initialize the Voice RAG system and Summarizer once and cache them\"\"\"\n    global voice_rag_system, summarizer\n    if voice_rag_system is None:\n        try:\n            voice_rag_system = VoiceRAGSystem(asr_model_size=\"small\")\n        except Exception as e:\n            logger.error(f\"Failed to initialize Voice RAG system: {e}\")\n            raise ValueError(f\"Failed to initialize Voice RAG system: {e}\")\n    \n    if summarizer is None:\n        try:\n            summarizer = DocumentSummarizer()\n        except Exception as e:\n            logger.error(f\"Failed to initialize Document Summarizer: {e}\")\n            summarizer = None\n    \n    return voice_rag_system, summarizer\n\ndef process_audio(audio_file, num_results):\n    \"\"\"Process audio file through the Voice RAG system\"\"\"\n    try:\n        # Initialize the systems if not already done\n        system, _ = initialize_systems()\n        \n        # Process the audio query\n        results = system.process_audio_query(audio_file, n_results=num_results)\n        \n        # Get transcription and answer\n        transcription = results[\"transcription\"]\n        answer = results[\"answer\"]\n        \n        # Format retrieved documents\n        retrieved_docs = \"\"\n        for i, doc in enumerate(results[\"retrieved_docs\"]):\n            retrieved_docs += f\"**Document {i+1}:** Source: {doc['source']}\\n\\n\"\n            retrieved_docs += f\"{doc['content'][:300]}...\\n\\n\"\n            retrieved_docs += \"---\\n\\n\"\n        \n        return transcription, answer, retrieved_docs\n    \n    except Exception as e:\n        logger.error(f\"Error processing audio: {e}\")\n        return f\"Error: {str(e)}\", \"Failed to generate answer\", \"No documents retrieved\"\n\ndef process_text_query(query, num_results):\n    \"\"\"Process a text query through the RAG QA system\"\"\"\n    try:\n        # Initialize the systems if not already done\n        system, _ = initialize_systems()\n        \n        # Process the query directly through the RAG QA system\n        results = system.rag_qa_system.process_query(query, n_results=num_results)\n        \n        # Get answer\n        answer = results[\"answer\"]\n        \n        # Format retrieved documents\n        retrieved_docs = \"\"\n        for i, doc in enumerate(results[\"retrieved_docs\"]):\n            retrieved_docs += f\"**Document {i+1}:** Source: {doc['source']}\\n\\n\"\n            retrieved_docs += f\"{doc['content'][:300]}...\\n\\n\"\n            retrieved_docs += \"---\\n\\n\"\n        \n        return answer, retrieved_docs\n    \n    except Exception as e:\n        logger.error(f\"Error processing query: {e}\")\n        return f\"Error: {str(e)}\", \"No documents retrieved\"\n\ndef generate_summary(retrieved_docs, max_length):\n    \"\"\"Generate a summary of the retrieved documents\"\"\"\n    try:\n        # Initialize the systems if not already done\n        _, sum_tool = initialize_systems()\n        \n        if not sum_tool:\n            return \"Summarization tool not available.\"\n        \n        # Parse retrieved documents from markdown\n        doc_texts = []\n        lines = retrieved_docs.split('\\n')\n        current_doc = {\"source\": \"\", \"content\": \"\"}\n        \n        for line in lines:\n            if line.startswith(\"**Document\"):\n                if current_doc[\"content\"]:\n                    doc_texts.append(current_doc)\n                    current_doc = {\"source\": \"\", \"content\": \"\"}\n                \n                # Extract source\n                source_match = re.search(r\"Source: (.*?)$\", line)\n                if source_match:\n                    current_doc[\"source\"] = source_match.group(1)\n            elif \"---\" not in line and current_doc[\"source\"]:\n                current_doc[\"content\"] += line + \" \"\n        \n        # Add the last document\n        if current_doc[\"content\"]:\n            doc_texts.append(current_doc)\n        \n        # Generate summary\n        if doc_texts:\n            summary = sum_tool.summarize_documents(doc_texts, max_length=int(max_length))\n            return summary\n        else:\n            return \"No documents to summarize.\"\n    \n    except Exception as e:\n        logger.error(f\"Error generating summary: {e}\")\n        return f\"Error generating summary: {str(e)}\"\n\n# ====== Create Gradio Interface ======\n\ndef create_interface():\n    \"\"\"\n    Create a Gradio interface for the Voice RAG system\n    \n    Returns:\n        Gradio interface\n    \"\"\"\n    # Define the interface\n    with gr.Blocks(title=\"Voice-Driven RAG System\") as demo:\n        gr.Markdown(\"# Voice-Interactive RAG System with Llama 2\")\n        gr.Markdown(\"\"\"\n        Ask questions about the PDFs using voice or text input.\n        \n        **Important**: To use Llama 2 for answer generation and summarization,\n        you need to set your Replicate API token in the code.\n        \"\"\")\n        \n        with gr.Tab(\"Voice Input\"):\n            audio_input = gr.Audio(type=\"filepath\", label=\"Record or Upload Audio\")\n            num_results_slider = gr.Slider(minimum=1, maximum=10, value=5, step=1, label=\"Number of Results\")\n            \n            with gr.Row():\n                submit_btn = gr.Button(\"Submit\")\n                clear_btn = gr.Button(\"Clear\")\n            \n            transcription_output = gr.Textbox(label=\"Transcribed Query\")\n            answer_output = gr.Textbox(label=\"Generated Answer\", lines=10)\n            docs_output = gr.Markdown(label=\"Retrieved Documents\")\n            \n            submit_btn.click(\n                process_audio, \n                inputs=[audio_input, num_results_slider],\n                outputs=[transcription_output, answer_output, docs_output]\n            )\n            \n            clear_btn.click(\n                lambda: (None, \"\", \"\", \"\"),\n                inputs=None,\n                outputs=[audio_input, transcription_output, answer_output, docs_output]\n            )\n        \n        with gr.Tab(\"Text Input\"):\n            text_input = gr.Textbox(label=\"Enter your question\", lines=2)\n            num_results_text = gr.Slider(minimum=1, maximum=10, value=5, step=1, label=\"Number of Results\")\n            \n            with gr.Row():\n                text_submit_btn = gr.Button(\"Submit\")\n                text_clear_btn = gr.Button(\"Clear\")\n            \n            text_answer_output = gr.Textbox(label=\"Generated Answer\", lines=10)\n            text_docs_output = gr.Markdown(label=\"Retrieved Documents\")\n            \n            text_submit_btn.click(\n                process_text_query,\n                inputs=[text_input, num_results_text],\n                outputs=[text_answer_output, text_docs_output]\n            )\n            \n            text_clear_btn.click(\n                lambda: (\"\", \"\", \"\"),\n                inputs=None,\n                outputs=[text_input, text_answer_output, text_docs_output]\n            )\n        \n        with gr.Tab(\"Document Summary\"):\n            with gr.Row():\n                summary_docs_input = gr.Markdown(label=\"Documents to Summarize\")\n                summary_length = gr.Slider(minimum=50, maximum=500, value=200, step=50, label=\"Summary Length (max tokens)\")\n            \n            with gr.Row():\n                summary_btn = gr.Button(\"Generate Summary\")\n                summary_clear_btn = gr.Button(\"Clear\")\n            \n            summary_output = gr.Textbox(label=\"Generated Summary\", lines=10)\n            \n            summary_btn.click(\n                generate_summary,\n                inputs=[summary_docs_input, summary_length],\n                outputs=[summary_output]\n            )\n            \n            summary_clear_btn.click(\n                lambda: (\"\", \"\"),\n                inputs=None,\n                outputs=[summary_docs_input, summary_output]\n            )\n        \n        gr.Markdown(\"### About This System\")\n        gr.Markdown(\"\"\"\n        This system combines:\n        1. **Speech Recognition** (Whisper) for transcribing voice queries\n        2. **Vector Search** (ChromaDB) for retrieving relevant document chunks\n        3. **Text Generation** (Llama 2 via Replicate API) for producing grounded answers\n        4. **Document Summarization** for creating abstractive summaries of documents\n        \n        The system only answers based on information found in the documents.\n        \"\"\")\n    \n    # Download Whisper model proactively\n    logger.info(\"Pre-downloading Whisper model...\")\n    whisper.load_model(\"small\")\n    \n    return demo\n\n# Main execution\nif __name__ == \"__main__\":\n    # Create and launch the interface\n    demo = create_interface()\n    demo.launch(share=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T10:42:56.691032Z","iopub.execute_input":"2025-04-26T10:42:56.691655Z","iopub.status.idle":"2025-04-26T10:43:07.372335Z","shell.execute_reply.started":"2025-04-26T10:42:56.691628Z","shell.execute_reply":"2025-04-26T10:43:07.371570Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(fp, map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7862\n* Running on public URL: https://4ca8d49fe5d02b4715.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://4ca8d49fe5d02b4715.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(fp, map_location=device)\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}